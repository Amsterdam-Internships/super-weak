{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65f70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "import skweak\n",
    "import evaluation as evn\n",
    "import itertools\n",
    "import utils\n",
    "from skweak.spacy import ModelAnnotator\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "nlp = spacy.load(\"nl_core_news_md\")\n",
    "from datasets import Dataset, ClassLabel, Sequence \n",
    "import multiprocessing\n",
    "import bz2 \n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sophisticated-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODAY I'LL TRAIN SOME SPACY MODELS - LET'S SEE... SONAR-1 AND CONLL-2002? \n",
    "\n",
    "#I ALSO HAVE TO MAKE A PRETRAINED CONLL-2002 XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stretch-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Afghanistan', 'Albanië', 'Algerije', 'Amerika', 'Andorra', 'Angola', 'Antigua', 'Argentinië', 'Armenië', 'Australië', 'Oostenrijk',\n",
    "             'Azerbeidzjan', \"Bahama's\", 'Bahrein', 'Bangladesh', 'Barbados', 'Wit-Rusland', 'België', 'Belize', 'Benin', 'Bhutan',\n",
    "             'Bolivia', 'Bosnië-Herzegovina', 'Botswana', 'Brazilië', 'Brunei', 'Bulgarije', 'Burkina', 'Burundi', 'Cambodja', 'Kameroen',\n",
    "             'Canada', 'Kaapverdië', 'Centraal-Afrikaanse Republiek', 'Tsjaad', 'Chili', 'China', 'Colombia', 'Comoren', 'Congo', 'Costa Rica',\n",
    "             'Kroatië', 'Cuba', 'Cyprus', 'Tsjechië', 'Denemarken', 'Djibouti', 'Dominica', 'Dominicaanse Republiek', 'Oost-Timor',\n",
    "             'Ecuador', 'Egypte', 'El Salvador', 'Equatoriaal-Guinea', 'Eritrea', 'Estland', 'Ethiopië', 'Fiji', 'Finland', 'Frankrijk',\n",
    "             'Gabon', 'Gambia', 'Georgië', 'Duitsland', 'Ghana', 'Griekenland', 'Grenada', 'Guatemala', 'Guinee', 'Guinee-Bissau', 'Guyana',\n",
    "             'Haïti', 'Honduras', 'Hongarije', 'IJsland', 'India', 'Indonesië', 'Iran', 'Irak', 'Ierland', 'Israël', 'Italië', 'Ivoorkust',\n",
    "             'Jamaica', 'Japan', 'Jordanië', 'Kazachstan', 'Kenia', 'Kiribati', 'Noord-Korea', 'Zuid-Korea', 'Kosovo', 'Koeweit', 'Kirgizië',\n",
    "             'Laos', 'Letland', 'Libanon', 'Lesotho', 'Liberia', 'Libië', 'Liechtenstein', 'Litouwen', 'Luxemburg', 'Macedonië', 'Madagaskar',\n",
    "             'Malawi', 'Maleisië', 'Maldiven', 'Mali', 'Malta', 'Marshalleilanden', 'Mauritanië', 'Mauritius', 'Mexico', 'Micronesië',\n",
    "             'Moldavië', 'Monaco', 'Mongolië', 'Montenegro', 'Marokko', 'Mozambique', 'Myanmar', 'Namibië', 'Nauru', 'Nepal', 'Nederland',\n",
    "             'Nieuw-Zeeland', 'Nicaragua', 'Niger', 'Nigeria', 'Noorwegen', 'Oman', 'Pakistan', 'Palau', 'Panama', 'Papoea-Nieuw-Guinea',\n",
    "             'Paraguay', 'Peru', 'Filipijnen', 'Polen', 'Portugal', 'Qatar', 'Roemenië', 'Russische Federatie', 'Rwanda', 'St Kitts & Nevis',\n",
    "             'St Lucia', 'Saint Vincent en de Grenadines', 'Samoa', 'San Marino', 'Sao Tome & Principe', 'Saudi-Arabië', 'Senegal', 'Servië',\n",
    "             'Seychellen', 'Sierra Leone', 'Singapore', 'Slowakije', 'Slovenië', 'Salomonseilanden', 'Somalië', 'Zuid-Afrika', 'Zuid-Soedan',\n",
    "             'Spanje', 'Sri Lanka', 'Soedan', 'Suriname', 'Swaziland', 'Zweden', 'Zwitserland', 'Syrië', 'Taiwan', 'Tadzjikistan', 'Tanzania',\n",
    "             'Thailand', 'Togo', 'Tonga', 'Trinidad en Tobago', 'Tunesië', 'Turkije', 'Turkmenistan', 'Tuvalu', 'Oeganda', 'Oekraïne',\n",
    "             'Verenigde Arabische Emiraten', 'Verenigd Koninkrijk', 'Verenigde Staten', 'Uruguay', 'Oezbekistan', 'Vanuatu', 'Vaticaanstad', 'Venezuela',\n",
    "             'Vietnam', 'Jemen', 'Zambia', 'Zimbabwe', 'VS', 'VK', 'Rusland', 'Zuid-Korea']\n",
    "\n",
    "langs = ['Afar', 'Abchazisch', 'Avestan', 'Afrikaans', 'Akan', 'Amhaars', 'Aragonese', 'Arabisch', 'Aramees', 'Assamees', 'Avarisch', 'Aymara',\n",
    "             'Azerbeidzjaans', 'Bashkir', 'Wit-Russisch', 'Bulgaars', 'Bambara', 'Bislama', 'Bengaals', 'Tibetaans', 'Bretons', 'Bosnisch', 'Kantonees',\n",
    "             'Catalaans', 'Tsjetsjeens', 'Chamorro', 'Corsicaans', 'Cree', 'Tsjechisch', 'Tsjoevasj', 'Welsh', 'Deens', 'Duits', 'Divehi', 'Dzongkha', 'Ooi ',\n",
    "             'Grieks', 'Engels', 'Esperanto', 'Spaans', 'Castiliaans', 'Ests', 'Baskisch', 'Perzisch', 'Fulah', 'Filipijns', 'Fins', 'Fijisch', 'Faeröers ',\n",
    "             'Frans', 'West-Fries', 'Iers', 'Gaelic', 'Galicisch', 'Guarani', 'Gujarati', 'Manx', 'Hausa', 'Hebreeuws', 'Hindi', 'Hiri Motu',\n",
    "             'Kroatisch', 'Haïtiaans', 'Hongaars', 'Armeens', 'Herero', 'Indonesisch', 'Igbo', 'Inupiaq', 'Ido', 'IJslands', 'Italiaans', 'Inuktitut',\n",
    "             'Japans', 'Javaans', 'Georgisch', 'Kongo', 'Kikuyu', 'Kuanyama', 'Kazachs', 'Kalaallisut', 'Groenlands', 'Centraal Khmer', 'Kannada',\n",
    "             'Koreaans', 'Kanuri', 'Kasjmiri', 'Koerdisch', 'Komi', 'Cornish', 'Kirgizisch', 'Latijn', 'Luxemburgs', 'Ganda', 'Limburgs', 'Lingala', 'Lao ',\n",
    "             'Litouws', 'Luba-Katanga', 'Lets', 'Malagasi', 'Marshallese', 'Maori', 'Macedonisch', 'Malayalam', 'Mongools', 'Marathi', 'Maleis', 'Maltees', 'Birmaans', 'Nauru', 'Bokmål', 'Noors', 'Ndebele', 'Nepalees', 'Ndonga', 'Nederlands', 'Vlaams', 'Nynorsk', 'Navajo', 'Chichewa',\n",
    "             'Occitaans', 'Ojibwa', 'Oromo', 'Oriya', 'Ossetisch', 'Punjabi', 'Pali', 'Pools', 'Pasjtoe', 'Portugees', 'Quechua', 'Reto-Romaans', 'Rundi ',\n",
    "             'Roemeens', 'Russisch', 'Kinyarwanda', 'Sanskriet', 'Sardinisch', 'Sindhi', 'Sami', 'Sango', 'Singalees', 'Slowaaks', 'Sloveens', 'Samoaans',\n",
    "             'Shona', 'Somalisch', 'Albanees', 'Servisch', 'Swati', 'Sotho', 'Soendanees', 'Zweeds', 'Swahili', 'Tamil', 'Telugu', 'Tadzjieks', 'Thais ',\n",
    "             'Tigrinya', 'Turkmeens', 'Taiwanees', 'Tagalog', 'Tswana', 'Tonga', 'Turks', 'Tsonga', 'Tataars', 'Twi', 'Tahitiaans', 'Oeigoers', 'Oekraïens ',\n",
    "             'Urdu', 'Oezbeeks', 'Venda', 'Vietnamees', 'Volapük', 'Wallonië', 'Wolof', 'Xhosa', 'Jiddisch', 'Yoruba', 'Zhuang', 'Mandarijn',\n",
    "             'Mandarijn Chinees', 'Chinees', 'Zulu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "structured-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = evn.to_spacy(\"data/ned_train.spacy\")\n",
    "val = evn.to_spacy(\"data/ned_testa.spacy\")\n",
    "test = evn.to_spacy(\"data/ned_testb.spacy\")\n",
    "sample = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "varying-links",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "designed-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(ent_label: str) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 label of Spacy labelled entity\n",
    "  \"\"\"\n",
    "  mappings = {\"PERSON\":\"PER\", \"COMPANY\":\"ORG\", \"GPE\":\"LOC\", 'EVENT':\"MISC\", 'FAC':\"MISC\", 'LANGUAGE':\"MISC\", 'LAW':\"MISC\", 'NORP':\"MISC\", 'PRODUCT':\"MISC\",'WORK_OF_ART':\"MISC\", \"MISC\":\"MISC\", \"PER\":\"PER\", \"ORG\":\"ORG\", \"LOC\":\"LOC\"}    \n",
    "  exclude = {\"CARDINAL\", \"ORDINAL\", \"DATE\", \"PERCENT\", \"QUANTITY\", \"TIME\", \"MONEY\"}\n",
    "\n",
    "  return mappings[ent_label] if ent_label != \"\" and ent_label not in exclude else \"O\"\n",
    "\n",
    "class RelabelledModelAnnotator(ModelAnnotator):\n",
    "  def __init__(self, name: str, model_path: str):\n",
    "    super(RelabelledModelAnnotator, self).__init__(name, model_path)\n",
    "    \n",
    "  def find_spans(self, doc):\n",
    "    # Create a new document (to avoid conflicting annotations)\n",
    "    doc2 = self.create_new_doc(doc)\n",
    "    # And run the model\n",
    "    for _, proc in self.model.pipeline:\n",
    "        doc2 = proc(doc2)\n",
    "    # Add the annotation\n",
    "    for ent in doc2.ents:\n",
    "        #just put relabel function here\n",
    "        yield ent.start, ent.end, relabel(ent.label_)\n",
    "        \n",
    "def country_annotator(doc: Doc):\n",
    "  for tok in doc:\n",
    "    if tok.text in countries:\n",
    "      yield tok.i, tok.i+1, \"LOC\"\n",
    "      \n",
    "def nationality_annotator(doc: Doc):\n",
    "  for tok in doc:\n",
    "    if tok.text in countries:\n",
    "      yield tok.i, tok.i+1, \"MISC\"\n",
    "      \n",
    "country_annotate = skweak.heuristics.FunctionAnnotator(\"countries\", country_annotator)\n",
    "nationality_annotate = skweak.heuristics.FunctionAnnotator(\"nationality\", nationality_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "british-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model annotators\n",
    "spacy_nl = RelabelledModelAnnotator(\"spacy\", \"nl_core_news_md\")\n",
    "spacy_conll = RelabelledModelAnnotator(\"conll\", \"models/conll2002_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "engaged-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/geonames.json\n",
      "Populating trie for class LOC (number: 15205)\n",
      "Extracting data from data/nl.json\n",
      "Populating trie for class LOC (number: 22819)\n",
      "Extracting data from data/crunchbase_alt.json\n",
      "Populating trie for class PER (number: 1062669)\n",
      "Populating trie for class ORG (number: 789205)\n"
     ]
    }
   ],
   "source": [
    "#gazetteers \n",
    "geonames = skweak.gazetteers.extract_json_data(\"data/geonames.json\")\n",
    "geonames_annotator = skweak.gazetteers.GazetteerAnnotator(\"geonames\", geonames)\n",
    "\n",
    "nederlocs = skweak.gazetteers.extract_json_data(\"data/nl.json\")\n",
    "dutch_loc_annotator = skweak.gazetteers.GazetteerAnnotator(\"nederlocs\", nederlocs)\n",
    "\n",
    "crunchbase = skweak.gazetteers.extract_json_data(\"data/crunchbase_alt.json\")\n",
    "crunchbase_annotator = skweak.gazetteers.GazetteerAnnotator(\"crunchbase\", crunchbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "material-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions \n",
    "import json\n",
    "import spacy_wrapper\n",
    "\n",
    "NAME_PREFIXES = {\"-\", \"von\", \"van\", \"de\", \"di\", \"le\", \"la\", \"het\", \"'t'\", \"dem\", \"der\", \"den\", \"d'\", \"ter\"}\n",
    "\n",
    "class SpanGenerator:\n",
    "    \"\"\"Generate spans that satisfy a token-level constratint. From Lison et al. 2020\"\"\"\n",
    "    \n",
    "    def __init__(self, constraint, label=\"ENT\", exceptions=(\"'s\", \"-\")):\n",
    "        \"\"\"annotation with a constraint (on spacy tokens). Exceptions are sets of tokens that are allowed\n",
    "        to violate the constraint inside the span\"\"\"\n",
    "        \n",
    "        self.constraint = constraint\n",
    "        self.label = label\n",
    "        self.exceptions = set(exceptions)\n",
    "        \n",
    "    def __call__(self, spacy_doc):    \n",
    "\n",
    "        i = 0\n",
    "        while i < len(spacy_doc):\n",
    "            tok = spacy_doc[i]\n",
    "                # We search for the longest span that satisfy the constraint\n",
    "            if self.constraint(tok):\n",
    "                j = i+1\n",
    "                while True:\n",
    "                    if j < len(spacy_doc) and self.constraint(spacy_doc[j]):\n",
    "                        j += 1\n",
    "                    # We relax the constraint a bit to allow genitive and dashes\n",
    "                    elif j < (len(spacy_doc)-1) and spacy_doc[j].text in self.exceptions and self.constraint(spacy_doc[j+1]):\n",
    "                        j += 2\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # To avoid too many FPs, we only keep entities with at least 3 characters (excluding punctuation)\n",
    "                if len(spacy_doc[i:j].text.rstrip(\".\")) > 2:\n",
    "                    yield i, j, self.label\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "class FullNameGenerator:\n",
    "    \"\"\"Search for occurrences of full person names (first name followed by at least one title token). From Lison et al. 2020\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        fd = open(\"data/first_names.json\")\n",
    "        self.first_names = set(json.load(fd))\n",
    "        fd.close()\n",
    "        self.suggest_generator = SpanGenerator(lambda x: is_likely_proper(x), \n",
    "                                               exceptions=NAME_PREFIXES)\n",
    "        \n",
    "    def __call__(self, spacy_doc):\n",
    "        for start, end, _ in self.suggest_generator(spacy_doc):  \n",
    "            # We assume full names are between 2 and 4 tokens\n",
    "            if (end-start) < 2 or (end-start) > 5:\n",
    "                continue\n",
    "                \n",
    "            elif (spacy_doc[start].text in self.first_names and spacy_doc[end-1].is_alpha \n",
    "                  and spacy_doc[end-1].is_title): \n",
    "                yield start, end, \"PER\"\n",
    "\n",
    "def in_compound(tok):\n",
    "    \"\"\"Returns true if the spacy token is part of a compound phrase\"\"\"\n",
    "    if tok.dep_==\"compound\":\n",
    "        return True\n",
    "    elif tok.i > 0 and tok.nbor(-1).dep_==\"compound\":\n",
    "        return True\n",
    "    return False\n",
    "  \n",
    "def is_likely_proper(tok):\n",
    "    \"\"\"Returns true if the spacy token is a likely proper name, based on its form.\"\"\"\n",
    "    if len(tok)< 2:\n",
    "        return False\n",
    "    \n",
    "    # If the lemma is titled, just return True\n",
    "    elif tok.lemma_.istitle():\n",
    "        return True\n",
    "       \n",
    "    # Handling cases such as iPad\n",
    "    elif len(tok)>2 and tok.text[0].islower() and tok.text[1].isupper() and tok.text[2:].islower():\n",
    "        return True\n",
    "    \n",
    "    elif (tok.is_upper and tok.text not in spacy_wrapper.CURRENCY_CODES \n",
    "          and tok.text not in spacy_wrapper.NOT_NAMED_ENTITIES):\n",
    "        return True\n",
    "    \n",
    "    # Else, check whether the surface token is titled and is not sentence-initial\n",
    "    elif (tok.i > 0 and tok.is_title and not tok.is_sent_start and tok.nbor(-1).text not in {'\\'', '\"', '‘', '“', '”', '’'} \n",
    "          and not tok.nbor(-1).text.endswith(\".\")):\n",
    "        return True\n",
    "    return False  \n",
    "\n",
    "fullname = FullNameGenerator()\n",
    "fullname_annotator = skweak.heuristics.FunctionAnnotator(\"fullname_detector\", fullname)\n",
    "\n",
    "class NameGetter():\n",
    "  \n",
    "  def __init__(self):  \n",
    "    fd = open(\"data/first_names.json\")\n",
    "    self.first_names = set(json.load(fd))\n",
    "    fd.close()\n",
    "    \n",
    "  def __call__(self, spacy_doc):\n",
    "    for tok in spacy_doc:\n",
    "      if tok.is_title:\n",
    "        if tok.text in self.first_names:\n",
    "          yield tok.i, tok.i+1, \"PER\"\n",
    "\n",
    "def per(spacy_doc: Doc) -> List[str]:\n",
    "  \"\"\"\n",
    "  retrieve all per labels from spacy_doc\n",
    "  \"\"\"\n",
    "  per_annotators = [\"conll\"]\n",
    "  per_tokens = [token for annotator in per_annotators for token in spacy_doc.spans[annotator] if token.label_ == \"PER\"]\n",
    "  return per_tokens\n",
    "\n",
    "def last_namer(spacy_doc): \n",
    "  per_tokens = per(spacy_doc)\n",
    "  last_names = [per[1:].text for per in per_tokens if len(per.text.split(\" \")) > 1]\n",
    "  for tok in spacy_doc:\n",
    "    if tok.is_title and tok.text in last_names:\n",
    "      yield tok.i, tok.i+1, \"PER\"\n",
    "      \n",
    "def misc_finder(spacy_doc):\n",
    "  spans = [subspan for span in spacy_doc.spans.values() for subspan in span if subspan.label_ == \"MISC\"]\n",
    "  for span in spans:\n",
    "    if span.end < len(spacy_doc):\n",
    "      if span.doc[span.end].is_title:\n",
    "        yield span.end, span.end+1, \"MISC\"\n",
    "        \n",
    "name = NameGetter()\n",
    "name_annotator = skweak.heuristics.FunctionAnnotator(\"name_annotator\", name)\n",
    "\n",
    "misc_annotator = skweak.heuristics.FunctionAnnotator(\"misc_annotator\", misc_finder)\n",
    "\n",
    "lastname_annotator = skweak.heuristics.FunctionAnnotator(\"lastname_annotator\", last_namer)\n",
    "lastname_annotator2 = skweak.heuristics.FunctionAnnotator(\"lastname_annotator2\", last_namer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "outer-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document level annotators\n",
    "mv = skweak.aggregation.MajorityVoter(\"mv\", [\"LOC\", \"MISC\", \"ORG\", \"PER\"])\n",
    "mv.add_underspecified_label(\"ENT\", {\"LOC\", \"MISC\", \"ORG\", \"PER\"})\n",
    "\n",
    "doc_hist = skweak.doclevel.DocumentHistoryAnnotator(\"doc_history\", \"mv\", [\"PER\", \"ORG\"])\n",
    "\n",
    "consistor = skweak.doclevel.DocumentMajorityAnnotator(\"doc_majority\", \"mv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "clear-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc: Doc) -> Doc:\n",
    "  #apply all labelling functions to doc\n",
    "  #return mv(name_annotator(nationality_annotate(country_annotate(spacy_conll(dutch_loc_annotator(geonames_annotator(crunchbase_annotator(fullname_annotator(spacy_nl(doc))))))))))\n",
    "  #return country_annotate(doc)\n",
    "  #return #dutch_loc_annotator(crunchbase_annotator(fullname_annotator(doc)))\n",
    "  doc = misc_annotator(lastname_annotator(crunchbase_annotator(geonames_annotator(dutch_loc_annotator(country_annotate(spacy_conll(doc)))))))\n",
    "  if np.any(list(doc.spans.values())):\n",
    "    doc = doc_hist(mv(doc))\n",
    "    return doc\n",
    "  #return spacy_conll(doc)\n",
    "\n",
    "def process_docs(docs: List[Doc]) -> Doc:\n",
    "  docs = [process_doc(doc) for doc in tqdm.notebook.tqdm(docs) if process_doc(doc)]\n",
    "  hmm = skweak.aggregation.HMM(\"hmm\", [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "  hmm.fit_and_aggregate(docs)\n",
    "  for doc in docs:\n",
    "    doc.ents = doc.spans['hmm']\n",
    "  return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "directed-romania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7dab8944b84448a41d079131faf385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Finished E-step with 1 documents\n",
      "Starting iteration 2\n",
      "Finished E-step with 1 documents\n",
      "Starting iteration 3\n",
      "Finished E-step with 1 documents\n",
      "Starting iteration 4\n",
      "Finished E-step with 1 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1        -117.3341             +nan\n",
      "         2        -110.7963          +6.5378\n",
      "         3        -108.1633          +2.6330\n",
      "         4        -107.4316          +0.7317\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hoi ik ben \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    William\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bos\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", Tweede Wereldoorlog, AUBREY, 23 jaar en ik woon in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amsterdam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " maar ben nu in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Heerenveen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Ik ga naar de Universiteit van \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amsterdam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Bos gaat naar de universiteit. Bos woont in de stad. Ik heet \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    William\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Hopelijk triggered dit de annotator. \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    William\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nederland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amerika\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    België\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = nlp(\"Hoi ik ben William Bos, Tweede Wereldoorlog, AUBREY, 23 jaar en ik woon in Amsterdam maar ben nu in Heerenveen. Ik ga naar de Universiteit van Amsterdam. Bos gaat naar de universiteit. Bos woont in de stad. Ik heet William. Hopelijk triggered dit de annotator. William. Nederland, Amerika, België.\")\n",
    "\n",
    "processed = process_docs([sample])\n",
    "  \n",
    "skweak.utils.display_entities(processed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "clear-habitat",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "train = evn.to_spacy(\"data/ned_train.spacy\")\n",
    "val = evn.to_spacy(\"data/ned_testa.spacy\")\n",
    "test = evn.to_spacy(\"data/ned_testb.spacy\")\n",
    "\n",
    "mapper = {\"O\" : 0,\n",
    "          \"PER\" : 1, \n",
    "          \"LOC\" : 2, \n",
    "          \"ORG\" : 3,\n",
    "          \"MISC\" : 4}\n",
    "\n",
    "def ent_iob(tok: Token) -> str:\n",
    "  \"\"\"\n",
    "  returns ent IOB label\n",
    "  \"\"\"\n",
    "  ent_type = \"-\" + tok.ent_type_ if tok.ent_type else \"\"\n",
    "  return tok.ent_iob_ + ent_type\n",
    "\n",
    "def ent_type(tok: Token) -> str:\n",
    "  \"\"\"\n",
    "  returns entity type for given token\n",
    "  \"\"\"\n",
    "  return tok.ent_type_ if tok.ent_type_ else tok.ent_iob_\n",
    "\n",
    "def doc_ents(doc: Doc) -> List[str]:\n",
    "  \"\"\"\n",
    "  returns list of IOB style formatted entities in doc\n",
    "  \"\"\"\n",
    "  return [ent_iob(tok) for tok in doc]\n",
    "  \n",
    "def score_annotator(docs: List[Doc]) -> dict:\n",
    "  \"\"\"\n",
    "  given docs, applies annotator to docs and evaluates against ground truth \n",
    "  \"\"\"\n",
    "  true = [token for doc in docs for token in doc_ents(doc)]\n",
    "  print(len(true))\n",
    "  \n",
    "  annotated = process_docs(docs)\n",
    "  pred = [token for doc in annotated for token in doc_ents(doc)]\n",
    "  print(len(pred))\n",
    "  \n",
    "  #calculate precision, recall, f1 and give it weights to micro average\n",
    "  #OR add all predictions to one humongous list and run functions on that list \n",
    "\n",
    "  print(\"Scoring...\")\n",
    "  labels = [\"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\", \"B-MISC\", \"I-MISC\"]\n",
    "  total = (precision_score(true, pred, labels=labels, average=\"micro\"), \n",
    "           recall_score(true, pred, labels=labels, average=\"micro\"),\n",
    "           f1_score(true, pred, labels=labels, average=\"micro\"))\n",
    "  results = {label : (precision_score(true, pred, pos_label=label, labels=[pos_label]), recall_score(true, pred, pos_label=label), f1_score(true, pred, pos_label=label)) for label in labels}\n",
    "  results[\"all\"] = total\n",
    "  return results\n",
    "\n",
    "\n",
    "sample = val\n",
    "score_annotator(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "superb-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded corpus, containing 1000682 tokens in 5897 documents.\n",
      "Converting docs from str to Spacy Doc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c493951c988a4ff1aff4dce5273cbd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering docs with no entities...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-2a838c4dd650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Filtering docs with no entities...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mnos_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessed\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done! Filtered\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnos_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"documents.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"nos.pkl\", \"rb\") as file:\n",
    "  corpus = pickle.load(file)\n",
    "  print(\"Loaded corpus, containing\", corpus[\"tokens\"], \"tokens in\", corpus[\"docs\"], \"documents.\")\n",
    "  print(\"Converting docs from str to Spacy Doc...\")\n",
    "  processed = tqdm.notebook.tqdm(nlp.pipe(corpus[\"texts\"]))\n",
    "  print(\"Filtering docs with no entities...\")\n",
    "  nos_docs = [doc for doc in processed if doc.ents != ()]\n",
    "  print(\"Done! Filtered\", len(processed) - len(nos_docs), \"documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "small-miller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5828"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nos_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sunrise-session",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611ab7e0678f4cbca43eb675c4eb5db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Finished E-step with 5799 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -644905.8506             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Finished E-step with 5799 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2     -621080.2408      +23825.6098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Finished E-step with 5799 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3     -616042.0792       +5038.1615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Finished E-step with 5799 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4     -614349.3850       +1692.6942\n"
     ]
    }
   ],
   "source": [
    "def relabel(ent_label: str) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 label of Spacy labelled entity\n",
    "  \"\"\"\n",
    "  mappings = {\"PERSON\":\"PER\", \"COMPANY\":\"ORG\", \"GPE\":\"LOC\", 'EVENT':\"MISC\", 'FAC':\"MISC\", 'LANGUAGE':\"MISC\", 'LAW':\"MISC\", 'NORP':\"MISC\", 'PRODUCT':\"MISC\",'WORK_OF_ART':\"MISC\", \"MISC\":\"MISC\", \"PER\":\"PER\", \"ORG\":\"ORG\", \"LOC\":\"LOC\"}    \n",
    "  exclude = {\"CARDINAL\", \"ORDINAL\", \"DATE\", \"PERCENT\", \"QUANTITY\", \"TIME\", \"MONEY\"}\n",
    "\n",
    "  return mappings[ent_label] if ent_label != \"\" and ent_label not in exclude else None\n",
    "\n",
    "def convert_ent(token) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 IOB style entity label of Spacy token\n",
    "  \"\"\"\n",
    "  return token.ent_iob_ + \"-\" + relabel(token.ent_type_) if relabel(token.ent_type_) else \"O\"\n",
    "\n",
    "def dataset_from_strings(docs: List[str]) -> Dataset:\n",
    "  assert isinstance(docs, list)\n",
    "  assert isinstance(docs[0], str)\n",
    "  spacy_docs = list(nlp.pipe(docs))\n",
    "  print(spacy_docs[0])\n",
    "  \n",
    "  processed = process_docs(spacy_docs)\n",
    "  \n",
    "def process_spacy(docs: list):\n",
    "  store = []\n",
    "  tokens = []\n",
    "  ids = []\n",
    "\n",
    "  c = 0\n",
    "  classlabels = ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'])\n",
    "  \n",
    "  processed = process_docs(docs)\n",
    "  \n",
    "  for doc in processed: \n",
    "    ents = [classlabels.str2int(convert_ent(tok)) for tok in doc]\n",
    "    toks = [token.text for token in doc]\n",
    "    store.append(ents)\n",
    "    tokens.append(toks)\n",
    "    ids.append(str(c))\n",
    "    c += 1 \n",
    "    \n",
    "  d = {\"ids\" : ids,\n",
    "       \"ner_tags\" : store,\n",
    "       \"tokens\" : tokens}\n",
    "\n",
    "  class_sequence = Sequence(feature =  classlabels, id = None)\n",
    "  ds = Dataset.from_dict(d)\n",
    "  ds.features[\"ner_tags\"] = class_sequence\n",
    "  return ds\n",
    "\n",
    "dataset = process_spacy(nos_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "specific-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = bz2.BZ2File(\"data/nos_hmm.bz2\", 'w')\n",
    "\n",
    "pickle.dump(dataset, target_path)\n",
    "\n",
    "target_path.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "functional-ebony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': '0',\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'tokens': ['In',\n",
       "  'de',\n",
       "  'Mexicaanse',\n",
       "  'stad',\n",
       "  'Ciudad',\n",
       "  'Juárez',\n",
       "  'hebben',\n",
       "  'gewapende',\n",
       "  'mannen',\n",
       "  'een',\n",
       "  'bloedbad',\n",
       "  'aangericht',\n",
       "  'onder',\n",
       "  'feestende',\n",
       "  'scholieren',\n",
       "  '.',\n",
       "  'De',\n",
       "  'circa',\n",
       "  '15',\n",
       "  'schutters',\n",
       "  'kwamen',\n",
       "  'aanrijden',\n",
       "  'in',\n",
       "  \"SUV's\",\n",
       "  ',',\n",
       "  'gingen',\n",
       "  'de',\n",
       "  'woning',\n",
       "  'binnen',\n",
       "  'waar',\n",
       "  'het',\n",
       "  'verjaardagsfeest',\n",
       "  'aan',\n",
       "  'de',\n",
       "  'gang',\n",
       "  'was',\n",
       "  'en',\n",
       "  'beschoten',\n",
       "  'de',\n",
       "  'aanwezigen',\n",
       "  '.']}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "objective-artist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "false\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check(doc):\n",
    "  if np.any(list(doc.spans.values())):\n",
    "    print(\"true\")\n",
    "    return doc\n",
    "  else: \n",
    "    print(\"false\")\n",
    "  \n",
    "def run_check(docs):\n",
    "  checked = [check(doc) for doc in docs if check(doc)]\n",
    "  return checked\n",
    "\n",
    "len(run_check(nos_docs[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "protecting-passion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1a50fa46cd4da89f61bc04c5679f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-18a0be1f59d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#dataset = process_spacy(corpus[\"texts\"][0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_spacy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-b43460e1f37f>\u001b[0m in \u001b[0;36mprocess_spacy\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[0mpiped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpiped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[0mprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpiped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#   for doc in processed:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d9d6e56c3e72>\u001b[0m in \u001b[0;36mprocess_docs\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m   \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprocess_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m   \u001b[0mhmm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskweak\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hmm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"PER\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ORG\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LOC\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MISC\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0mhmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_and_aggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d9d6e56c3e72>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m   \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprocess_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m   \u001b[0mhmm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskweak\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hmm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"PER\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ORG\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LOC\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MISC\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0mhmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_and_aggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d9d6e56c3e72>\u001b[0m in \u001b[0;36mprocess_doc\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;31m#return country_annotate(doc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;31m#return #dutch_loc_annotator(crunchbase_annotator(fullname_annotator(doc)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mdoc_hist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlastname_annotator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrunchbase_annotator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeonames_annotator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdutch_loc_annotator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountry_annotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspacy_conll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m   \u001b[1;31m#return spacy_conll(doc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wpnbo\\miniconda3\\lib\\site-packages\\skweak\\aggregation.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;31m# Running the actual aggregation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0magg_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"O\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wpnbo\\miniconda3\\lib\\site-packages\\skweak\\aggregation.py\u001b[0m in \u001b[0;36m_aggregate\u001b[1;34m(self, obs, coefficient)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_obs_to_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mlabel_votes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m# For token-level segmentation (with a special O label), the number of \"O\" predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wpnbo\\miniconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;34m'Cannot apply_along_axis when any iteration dimensions are 0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         ) from None\n\u001b[1;32m--> 379\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;31m# build a buffer for storing evaluations of func1d.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wpnbo\\miniconda3\\lib\\site-packages\\skweak\\aggregation.py\u001b[0m in \u001b[0;36mcount_function\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmin_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_obs_to_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mlabel_votes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbincount\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "#dataset = process_spacy(corpus[\"texts\"][0])\n",
    "dataset = process_spacy(val[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "fitted-meter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb55694b2ae94bdfbedfb697bae551e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Finished E-step with 74 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1      -22927.7872             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 74 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2      -22180.8489        +746.9383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 74 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3      -22013.2271        +167.6218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 74 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4      -21941.2486         +71.9785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37687\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "docs = val \n",
    "\n",
    "true = [token for doc in docs for token in doc_ents(doc)]\n",
    "print(len(true))\n",
    "\n",
    "annotated = process_docs(docs)\n",
    "pred = [token for doc in annotated for token in doc_ents(doc)]\n",
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "leading-calculator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 33973,\n",
       "         'B-MISC': 748,\n",
       "         'B-LOC': 479,\n",
       "         'I-LOC': 64,\n",
       "         'B-ORG': 686,\n",
       "         'I-ORG': 396,\n",
       "         'B-PER': 703,\n",
       "         'I-PER': 423,\n",
       "         'I-MISC': 215})"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "preceding-trainer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 34141,\n",
       "         'B-MISC': 674,\n",
       "         'B-LOC': 467,\n",
       "         'I-LOC': 44,\n",
       "         'B-ORG': 765,\n",
       "         'I-ORG': 347,\n",
       "         'B-PER': 673,\n",
       "         'I-PER': 429,\n",
       "         'I-MISC': 147})"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "dental-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens = np.array([token for doc in val for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "corrected-adapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8336153412295544, 0.7959073774905762, 0.8143250688705234, None)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "labels = [\"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\", \"B-MISC\", \"I-MISC\"]\n",
    "#results = {label : (precision_score(true, pred, pos_label=label, labels=[label]), recall_score(true, pred, pos_label=label, labels=[label]), f1_score(true, pred, pos_label=label, label=label)) for label in labels}\n",
    "\n",
    "average = None\n",
    "\n",
    "# total = (precision_score(true, pred, labels=labels, average=average), \n",
    "#          recall_score(true, pred, labels=labels, average=average),\n",
    "#          f1_score(true, pred, labels=labels, average=average))\n",
    "\n",
    "precision_recall_fscore_support(true, pred, labels=labels, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "smart-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMISC_idx = [idx for idx, token in enumerate(true, 0) if token == \"I-MISC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "tracked-franchise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Leven, Tracability, Financiële, Morgen, 2000, 2000, Parade, 2000,\n",
       "       Belgique, soy, el, Diego, ben, Diego, 2002, Wereldoorlog, Leven,\n",
       "       Financiële, Morgen, 20-index, Leonese, Zomer, Zomer, van, de, Eeuw,\n",
       "       Journaal, Leven, onvoltooid, verleden, tijd, Financiële, Morgen,\n",
       "       Exchange, Baraks, jiu-jitsu, David, II, David, II, David, II, &,\n",
       "       Cosy, Brussel, Donna, Brussel, Contact, Digital, Audio,\n",
       "       Broadcasting, Mobile, Telecommunications, Standard, Financiële,\n",
       "       Morgen, Leven, Leven, Financiële, Morgen, Club, Reine, Margot, Of,\n",
       "       The, Gypsies, Dream, My, People, Vibes, And, One, Is, One, 250cc,\n",
       "       van, Vlaanderen, Quiz, van, de, Eeuw, Zomerzuidkant, Casablanca,\n",
       "       Brussel-T-shirt, Molenaar, Leven, Financiële, Morgen, van,\n",
       "       Denemarken, de, France, Spelen, Spelen, München, Cupvindt,\n",
       "       Vandenbrouckestelt, Cup, van, Frankrijk, 2000, Formule, 1, Prijzen,\n",
       "       kampioenschap, Opel, Lotus, F3-kampioen, 1, Prijs, 1, 3000,\n",
       "       1-loopbaan, F3-kampioen, 1, 1-zege, Prijs, Prijzen, Leven,\n",
       "       Financiële, Morgen, Markup, Language, Gidsen, Gids, Directories,\n",
       "       Financiële, Morgen, Leven, Feesten, d'Orient, to, Zimbabwe, van,\n",
       "       Antwerpen, Johnson, 2000, 2000, 2000, van, München, Spelen, Spelen,\n",
       "       Bollywood'-film, Bollywood'-films, Leven, UK, II, Wereldoorlog,\n",
       "       Leven, Financiële, Morgen, Leven, Loco, in, zijn, del, caballo,\n",
       "       grande, op, Borstvoeding, week, van, de, borstvoeding, week, van,\n",
       "       de, borstvoeding, Leven, FROM, THE, HILL, FICTION, BIG, NIGHT,\n",
       "       zonder, Grenzen, 6, 6, zonder, Grenzen-richtlijn, Economie,\n",
       "       European, Real, Estate, Fund, Financiële, Morgen, in, de, arena,\n",
       "       avec, un, fauteuil, Story, Vuur, ezel, with, broken, wings, avec,\n",
       "       un, fauteuil, of, ARD-exclusiv, Zeppelin-gitarist, Goghs,\n",
       "       Sterrennacht, Demoiselles, d'Avignon, Marilyn, Monroe, Yorkse,\n",
       "       Yorkse], dtype=object)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_MISC = val_tokens[IMISC_idx]\n",
    "I_MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset, ClassLabel, Sequence\n",
    "doc = nlp(sample.text)\n",
    "\n",
    "def name_detector_fn(doc):\n",
    "    names = [\"David\", \"Cameron\", \"William\", \"Bos\"]\n",
    "    for token in doc: \n",
    "        if token.text in names:\n",
    "            yield token.i, token.i+1, \"PER\"\n",
    "\n",
    "name_detector = skweak.heuristics.FunctionAnnotator(\"name_detector\", name_detector_fn)\n",
    "\n",
    "geonames = skweak.gazetteers.extract_json_data(\"data/geonames.json\", spacy_model=\"en_core_web_sm\")\n",
    "geonames_annotator = skweak.gazetteers.GazetteerAnnotator(\"locations\", geonames)\n",
    "\n",
    "\n",
    "#hmm = skweak.aggregation.HMM(\"hmm\", [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "#hmm.fit_and_aggregate([doc])\n",
    "#spans = doc.spans[\"hmm\"]\n",
    "#doc.ents = spans\n",
    "\n",
    "#for span in doc.ents:\n",
    "#  print(span, span.label, span.start, span.end, span.doc[span.start : span.end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(geonames_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, ClassLabel, Sequence\n",
    "\n",
    "def relabel(ent_label: str) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 label of Spacy labelled entity\n",
    "  \"\"\"\n",
    "  mappings = {\"PERSON\":\"PER\", \"COMPANY\":\"ORG\", \"GPE\":\"LOC\", 'EVENT':\"MISC\", 'FAC':\"MISC\", 'LANGUAGE':\"MISC\", 'LAW':\"MISC\", 'NORP':\"MISC\", 'PRODUCT':\"MISC\",'WORK_OF_ART':\"MISC\", \"MISC\":\"MISC\", \"PER\":\"PER\", \"ORG\":\"ORG\", \"LOC\":\"LOC\"}    \n",
    "  exclude = {\"CARDINAL\", \"ORDINAL\", \"DATE\", \"PERCENT\", \"QUANTITY\", \"TIME\", \"MONEY\"}\n",
    "\n",
    "  return mappings[ent_label] if ent_label != \"\" and ent_label not in exclude else None\n",
    "\n",
    "def convert_ent(token) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 IOB style entity label of Spacy token\n",
    "  \"\"\"\n",
    "  return token.ent_iob_ + \"-\" + relabel(token.ent_type_) if relabel(token.ent_type_) else token.ent_iob_\n",
    "\n",
    "def process_spacy(docs: list):\n",
    "  store = []\n",
    "  tokens = []\n",
    "  ids = []\n",
    "\n",
    "  c = 0\n",
    "  classlabels = ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'])\n",
    "  for doc in docs:\n",
    "    ents = [classlabels.str2int(convert_ent(tok)) for tok in doc]\n",
    "    toks = [token.text for token in doc]\n",
    "    store.append(ents)\n",
    "    tokens.append(toks)\n",
    "    ids.append(str(c))\n",
    "    c += 1 \n",
    "    \n",
    "  d = {\"ids\" : ids,\n",
    "       \"ner_tags\" : store,\n",
    "       \"tokens\" : tokens}\n",
    "\n",
    "  class_sequence = Sequence(feature =  classlabels, id = None)\n",
    "  ds = Dataset.from_dict(d)\n",
    "  ds.features[\"ner_tags\"] = class_sequence\n",
    "  return ds\n",
    "\n",
    "ds = process_spacy([doc])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b361823-c157-4bf7-ae16-4d3105914fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# We retrieve the texts from Wablieft tarfile\n",
    "texts = [] \n",
    "archive_file = tarfile.open(\"data/plain.tar.xz\")\n",
    "for archive_member in archive_file.getnames():\n",
    "    if archive_member.endswith(\".txt\"):\n",
    "        #weird encoding because of Turkish letters\n",
    "        text = archive_file.extractfile(archive_member).read().decode(\"cp850\")\n",
    "        texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc8d9ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/geonames.json\n",
      "Populating trie for class GPE (number: 15205)\n"
     ]
    }
   ],
   "source": [
    "def name_detector_fn(doc):\n",
    "    names = [\"David\", \"Cameron\", \"William\", \"Bos\"]\n",
    "    for token in doc: \n",
    "        if token.text in names:\n",
    "            yield token.i, token.i+1, \"PER\"\n",
    "\n",
    "name_detector = skweak.heuristics.FunctionAnnotator(\"name_detector\", name_detector_fn)\n",
    "\n",
    "def address_detector_fn(doc):\n",
    "    for token in doc: \n",
    "        if token.text[0].isupper() and doc[token.i+1].is_digit:\n",
    "            yield token.i, token.i+2, \"LOCATION\"\n",
    "\n",
    "address_detector = skweak.heuristics.FunctionAnnotator(\"address_detector\", address_detector_fn)\n",
    "\n",
    "def company_detector_fn(doc):\n",
    "    companies = [\"Microsoft\", \"Apple\", \"Gemeente Amsterdam\", \"Universiteit van Amsterdam\", \"UvA\"]\n",
    "    for token in doc:\n",
    "        if token.text in companies:\n",
    "            yield token.i, token.i+1, \"ORG\"\n",
    "\n",
    "company_detector = skweak.heuristics.FunctionAnnotator(\"company_detector\", company_detector_fn)\n",
    "\n",
    "names = skweak.gazetteers.extract_json_data(\"data/geonames.json\", spacy_model=\"en_core_web_sm\")\n",
    "name_annotator = skweak.gazetteers.GazetteerAnnotator(\"locations\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "naval-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(address_detector) == skweak.heuristics.FunctionAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = list(name_detector.pipe(name_annotator.pipe(docs[:25])))\n",
    "hmm = skweak.aggregation.HMM(\"hmm\", [\"PERSON\", \"LOCATION\", \"ORG\", \"GPE\"])\n",
    "\n",
    "#skweak.utils.display_entities(processed[7], \"locations\")\n",
    "hmm.fit_and_aggregate(processed)\n",
    "skweak.utils.display_entities(processed[7], \"hmm\", add_tooltip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb882579-f2b5-42b7-949b-acd14f6bc932",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in processed:\n",
    "    if \"hmm\" in doc.spans.keys():\n",
    "        doc.ents = doc.spans[\"hmm\"]\n",
    "    else: \n",
    "        doc.ents = ()\n",
    "\n",
    "docs = docs[:100]\n",
    "skweak.utils.docbin_writer(docs, \"data/wablieft.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e7409-4b44-4ca5-98c6-c1acc49e1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b9864-0173-49fa-b912-601574d3e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy train config.cfg --paths.train data/wablieft.spacy --paths.dev data/wablieft.spacy --initialize.vectors nl_core_news_md --output output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d60f36-2ee5-4ace-8004-cef7f35e33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load(\"output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4866f2-3b2b-4bea-914d-39e74acb1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = processed[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe064c-6b76-40e6-b25e-3b177976a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "skweak.utils.display_entities(model(\"Hoi ik ben William en ik woon in New York. Ik kom uit Flanders en ben geboren in Antwerpen.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990abbc5-e408-4044-8038-bacb93a0f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lison2020 import annotations\n",
    "from spacy.tokens import DocBin\n",
    "test = DocBin().from_disk(\"data/ned_testb.spacy\")\n",
    "sys.path.insert(0, './lison2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457143e0-71a1-42ea-9665-1b692bc93f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = annotations.FullAnnotator().add_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066d396-ada1-421c-9fef-35eb03d8e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598172f-9de8-483f-9a07-b35e0f721559",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.spacy_benchmark(\"data/ned_testb.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7940a0-c9f8-4149-a8df-f217bd693862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
