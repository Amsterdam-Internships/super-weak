{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d65f70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "import skweak\n",
    "import evaluation as evn\n",
    "import itertools\n",
    "from skweak.spacy import ModelAnnotator\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "nlp = spacy.load(\"nl_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sophisticated-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODAY I'LL TRAIN SOME SPACY MODELS - LET'S SEE... SONAR-1 AND CONLL-2002? \n",
    "\n",
    "#I ALSO HAVE TO MAKE A PRETRAINED CONLL-2002 XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "stretch-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Afghanistan', 'Albanië', 'Algerije', 'Andorra', 'Angola', 'Antigua', 'Argentinië', 'Armenië', 'Australië', 'Oostenrijk',\n",
    "             'Azerbeidzjan', \"Bahama's\", 'Bahrein', 'Bangladesh', 'Barbados', 'Wit-Rusland', 'België', 'Belize', 'Benin', 'Bhutan',\n",
    "             'Bolivia', 'Bosnië-Herzegovina', 'Botswana', 'Brazilië', 'Brunei', 'Bulgarije', 'Burkina', 'Burundi', 'Cambodja', 'Kameroen',\n",
    "             'Canada', 'Kaapverdië', 'Centraal-Afrikaanse Republiek', 'Tsjaad', 'Chili', 'China', 'Colombia', 'Comoren', 'Congo', 'Costa Rica',\n",
    "             'Kroatië', 'Cuba', 'Cyprus', 'Tsjechië', 'Denemarken', 'Djibouti', 'Dominica', 'Dominicaanse Republiek', 'Oost-Timor',\n",
    "             'Ecuador', 'Egypte', 'El Salvador', 'Equatoriaal-Guinea', 'Eritrea', 'Estland', 'Ethiopië', 'Fiji', 'Finland', 'Frankrijk',\n",
    "             'Gabon', 'Gambia', 'Georgië', 'Duitsland', 'Ghana', 'Griekenland', 'Grenada', 'Guatemala', 'Guinee', 'Guinee-Bissau', 'Guyana',\n",
    "             'Haïti', 'Honduras', 'Hongarije', 'IJsland', 'India', 'Indonesië', 'Iran', 'Irak', 'Ierland', 'Israël', 'Italië', 'Ivoorkust',\n",
    "             'Jamaica', 'Japan', 'Jordanië', 'Kazachstan', 'Kenia', 'Kiribati', 'Noord-Korea', 'Zuid-Korea', 'Kosovo', 'Koeweit', 'Kirgizië',\n",
    "             'Laos', 'Letland', 'Libanon', 'Lesotho', 'Liberia', 'Libië', 'Liechtenstein', 'Litouwen', 'Luxemburg', 'Macedonië', 'Madagaskar',\n",
    "             'Malawi', 'Maleisië', 'Maldiven', 'Mali', 'Malta', 'Marshalleilanden', 'Mauritanië', 'Mauritius', 'Mexico', 'Micronesië',\n",
    "             'Moldavië', 'Monaco', 'Mongolië', 'Montenegro', 'Marokko', 'Mozambique', 'Myanmar', 'Namibië', 'Nauru', 'Nepal', 'Nederland',\n",
    "             'Nieuw-Zeeland', 'Nicaragua', 'Niger', 'Nigeria', 'Noorwegen', 'Oman', 'Pakistan', 'Palau', 'Panama', 'Papoea-Nieuw-Guinea',\n",
    "             'Paraguay', 'Peru', 'Filipijnen', 'Polen', 'Portugal', 'Qatar', 'Roemenië', 'Russische Federatie', 'Rwanda', 'St Kitts & Nevis',\n",
    "             'St Lucia', 'Saint Vincent en de Grenadines', 'Samoa', 'San Marino', 'Sao Tome & Principe', 'Saudi-Arabië', 'Senegal', 'Servië',\n",
    "             'Seychellen', 'Sierra Leone', 'Singapore', 'Slowakije', 'Slovenië', 'Salomonseilanden', 'Somalië', 'Zuid-Afrika', 'Zuid-Soedan',\n",
    "             'Spanje', 'Sri Lanka', 'Soedan', 'Suriname', 'Swaziland', 'Zweden', 'Zwitserland', 'Syrië', 'Taiwan', 'Tadzjikistan', 'Tanzania',\n",
    "             'Thailand', 'Togo', 'Tonga', 'Trinidad en Tobago', 'Tunesië', 'Turkije', 'Turkmenistan', 'Tuvalu', 'Oeganda', 'Oekraïne',\n",
    "             'Verenigde Arabische Emiraten', 'Verenigd Koninkrijk', 'Verenigde Staten', 'Uruguay', 'Oezbekistan', 'Vanuatu', 'Vaticaanstad', 'Venezuela',\n",
    "             'Vietnam', 'Jemen', 'Zambia', 'Zimbabwe', 'VS', 'VK', 'Rusland', 'Zuid-Korea']\n",
    "\n",
    "langs = ['Afar', 'Abchazisch', 'Avestan', 'Afrikaans', 'Akan', 'Amhaars', 'Aragonese', 'Arabisch', 'Aramees', 'Assamees', 'Avarisch', 'Aymara',\n",
    "             'Azerbeidzjaans', 'Bashkir', 'Wit-Russisch', 'Bulgaars', 'Bambara', 'Bislama', 'Bengaals', 'Tibetaans', 'Bretons', 'Bosnisch', 'Kantonees',\n",
    "             'Catalaans', 'Tsjetsjeens', 'Chamorro', 'Corsicaans', 'Cree', 'Tsjechisch', 'Tsjoevasj', 'Welsh', 'Deens', 'Duits', 'Divehi', 'Dzongkha', 'Ooi ',\n",
    "             'Grieks', 'Engels', 'Esperanto', 'Spaans', 'Castiliaans', 'Ests', 'Baskisch', 'Perzisch', 'Fulah', 'Filipijns', 'Fins', 'Fijisch', 'Faeröers ',\n",
    "             'Frans', 'West-Fries', 'Iers', 'Gaelic', 'Galicisch', 'Guarani', 'Gujarati', 'Manx', 'Hausa', 'Hebreeuws', 'Hindi', 'Hiri Motu',\n",
    "             'Kroatisch', 'Haïtiaans', 'Hongaars', 'Armeens', 'Herero', 'Indonesisch', 'Igbo', 'Inupiaq', 'Ido', 'IJslands', 'Italiaans', 'Inuktitut',\n",
    "             'Japans', 'Javaans', 'Georgisch', 'Kongo', 'Kikuyu', 'Kuanyama', 'Kazachs', 'Kalaallisut', 'Groenlands', 'Centraal Khmer', 'Kannada',\n",
    "             'Koreaans', 'Kanuri', 'Kasjmiri', 'Koerdisch', 'Komi', 'Cornish', 'Kirgizisch', 'Latijn', 'Luxemburgs', 'Ganda', 'Limburgs', 'Lingala', 'Lao ',\n",
    "             'Litouws', 'Luba-Katanga', 'Lets', 'Malagasi', 'Marshallese', 'Maori', 'Macedonisch', 'Malayalam', 'Mongools', 'Marathi', 'Maleis', 'Maltees', 'Birmaans', 'Nauru', 'Bokmål', 'Noors', 'Ndebele', 'Nepalees', 'Ndonga', 'Nederlands', 'Vlaams', 'Nynorsk', 'Navajo', 'Chichewa',\n",
    "             'Occitaans', 'Ojibwa', 'Oromo', 'Oriya', 'Ossetisch', 'Punjabi', 'Pali', 'Pools', 'Pasjtoe', 'Portugees', 'Quechua', 'Reto-Romaans', 'Rundi ',\n",
    "             'Roemeens', 'Russisch', 'Kinyarwanda', 'Sanskriet', 'Sardinisch', 'Sindhi', 'Sami', 'Sango', 'Singalees', 'Slowaaks', 'Sloveens', 'Samoaans',\n",
    "             'Shona', 'Somalisch', 'Albanees', 'Servisch', 'Swati', 'Sotho', 'Soendanees', 'Zweeds', 'Swahili', 'Tamil', 'Telugu', 'Tadzjieks', 'Thais ',\n",
    "             'Tigrinya', 'Turkmeens', 'Taiwanees', 'Tagalog', 'Tswana', 'Tonga', 'Turks', 'Tsonga', 'Tataars', 'Twi', 'Tahitiaans', 'Oeigoers', 'Oekraïens ',\n",
    "             'Urdu', 'Oezbeeks', 'Venda', 'Vietnamees', 'Volapük', 'Wallonië', 'Wolof', 'Xhosa', 'Jiddisch', 'Yoruba', 'Zhuang', 'Mandarijn',\n",
    "             'Mandarijn Chinees', 'Chinees', 'Zulu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "structured-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = evn.to_spacy(\"data/ned_train.spacy\")\n",
    "val = evn.to_spacy(\"data/ned_testa.spacy\")\n",
    "test = evn.to_spacy(\"data/ned_testb.spacy\")\n",
    "sample = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "designed-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(ent_label: str) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 label of Spacy labelled entity\n",
    "  \"\"\"\n",
    "  mappings = {\"PERSON\":\"PER\", \"COMPANY\":\"ORG\", \"GPE\":\"LOC\", 'EVENT':\"MISC\", 'FAC':\"MISC\", 'LANGUAGE':\"MISC\", 'LAW':\"MISC\", 'NORP':\"MISC\", 'PRODUCT':\"MISC\",'WORK_OF_ART':\"MISC\", \"MISC\":\"MISC\", \"PER\":\"PER\", \"ORG\":\"ORG\", \"LOC\":\"LOC\"}    \n",
    "  exclude = {\"CARDINAL\", \"ORDINAL\", \"DATE\", \"PERCENT\", \"QUANTITY\", \"TIME\", \"MONEY\"}\n",
    "\n",
    "  return mappings[ent_label] if ent_label != \"\" and ent_label not in exclude else \"O\"\n",
    "\n",
    "class RelabelledModelAnnotator(ModelAnnotator):\n",
    "  def __init__(self, name: str, model_path: str):\n",
    "    super(RelabelledModelAnnotator, self).__init__(name, model_path)\n",
    "    \n",
    "  def find_spans(self, doc):\n",
    "    # Create a new document (to avoid conflicting annotations)\n",
    "    doc2 = self.create_new_doc(doc)\n",
    "    # And run the model\n",
    "    for _, proc in self.model.pipeline:\n",
    "        doc2 = proc(doc2)\n",
    "    # Add the annotation\n",
    "    for ent in doc2.ents:\n",
    "        #just put relabel function here\n",
    "        yield ent.start, ent.end, relabel(ent.label_)\n",
    "        \n",
    "def country_annotator(doc: Doc):\n",
    "  for tok in doc:\n",
    "    if tok.text in countries:\n",
    "      yield tok.i-1, tok.i+1, \"LOC\"\n",
    "      \n",
    "def nationality_annotator(doc: Doc):\n",
    "  for tok in doc:\n",
    "    if tok.text in countries:\n",
    "      yield tok.i-1, tok.i+1, \"MISC\"\n",
    "      \n",
    "country_annotate = skweak.heuristics.FunctionAnnotator(\"countries\", country_annotator)\n",
    "nationality_annotate = skweak.heuristics.FunctionAnnotator(\"nationality\", nationality_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "british-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model annotators\n",
    "spacy_nl = RelabelledModelAnnotator(\"spacy\", \"nl_core_news_md\")\n",
    "spacy_conll = RelabelledModelAnnotator(\"conll\", \"models/conll2002_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/geonames.json\n",
      "Populating trie for class LOC (number: 15205)\n",
      "Extracting data from data/nl.json\n",
      "Populating trie for class LOC (number: 22819)\n",
      "Extracting data from data/crunchbase_alt.json\n",
      "Populating trie for class PER (number: 1062669)\n",
      "Populating trie for class ORG (number: 789205)\n"
     ]
    }
   ],
   "source": [
    "#gazetteers \n",
    "geonames = skweak.gazetteers.extract_json_data(\"data/geonames.json\")\n",
    "geonames_annotator = skweak.gazetteers.GazetteerAnnotator(\"geonames\", geonames)\n",
    "\n",
    "nederlocs = skweak.gazetteers.extract_json_data(\"data/nl.json\")\n",
    "dutch_loc_annotator = skweak.gazetteers.GazetteerAnnotator(\"nederlocs\", nederlocs)\n",
    "\n",
    "crunchbase = skweak.gazetteers.extract_json_data(\"data/crunchbase_alt.json\")\n",
    "crunchbase_annotator = skweak.gazetteers.GazetteerAnnotator(\"crunchbase\", crunchbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "material-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions \n",
    "import json\n",
    "import spacy_wrapper\n",
    "\n",
    "NAME_PREFIXES = {\"-\", \"von\", \"van\", \"de\", \"di\", \"le\", \"la\", \"het\", \"'t'\", \"dem\", \"der\", \"den\", \"d'\", \"ter\"}\n",
    "\n",
    "class SpanGenerator:\n",
    "    \"\"\"Generate spans that satisfy a token-level constratint. From Lison et al. 2020\"\"\"\n",
    "    \n",
    "    def __init__(self, constraint, label=\"ENT\", exceptions=(\"'s\", \"-\")):\n",
    "        \"\"\"annotation with a constraint (on spacy tokens). Exceptions are sets of tokens that are allowed\n",
    "        to violate the constraint inside the span\"\"\"\n",
    "        \n",
    "        self.constraint = constraint\n",
    "        self.label = label\n",
    "        self.exceptions = set(exceptions)\n",
    "        \n",
    "    def __call__(self, spacy_doc):    \n",
    "\n",
    "        i = 0\n",
    "        while i < len(spacy_doc):\n",
    "            tok = spacy_doc[i]\n",
    "                # We search for the longest span that satisfy the constraint\n",
    "            if self.constraint(tok):\n",
    "                j = i+1\n",
    "                while True:\n",
    "                    if j < len(spacy_doc) and self.constraint(spacy_doc[j]):\n",
    "                        j += 1\n",
    "                    # We relax the constraint a bit to allow genitive and dashes\n",
    "                    elif j < (len(spacy_doc)-1) and spacy_doc[j].text in self.exceptions and self.constraint(spacy_doc[j+1]):\n",
    "                        j += 2\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # To avoid too many FPs, we only keep entities with at least 3 characters (excluding punctuation)\n",
    "                if len(spacy_doc[i:j].text.rstrip(\".\")) > 2:\n",
    "                    yield i, j, self.label\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "class FullNameGenerator:\n",
    "    \"\"\"Search for occurrences of full person names (first name followed by at least one title token). From Lison et al. 2020\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        fd = open(\"data/first_names.json\")\n",
    "        self.first_names = set(json.load(fd))\n",
    "        fd.close()\n",
    "        self.suggest_generator = SpanGenerator(lambda x: is_likely_proper(x), \n",
    "                                               exceptions=NAME_PREFIXES)\n",
    "        \n",
    "    def __call__(self, spacy_doc):\n",
    "        for start, end, _ in self.suggest_generator(spacy_doc):  \n",
    "            # We assume full names are between 2 and 4 tokens\n",
    "            if (end-start) < 2 or (end-start) > 5:\n",
    "                continue\n",
    "                \n",
    "            elif (spacy_doc[start].text in self.first_names and spacy_doc[end-1].is_alpha \n",
    "                  and spacy_doc[end-1].is_title): \n",
    "                yield start, end, \"PER\"\n",
    "\n",
    "def in_compound(tok):\n",
    "    \"\"\"Returns true if the spacy token is part of a compound phrase\"\"\"\n",
    "    if tok.dep_==\"compound\":\n",
    "        return True\n",
    "    elif tok.i > 0 and tok.nbor(-1).dep_==\"compound\":\n",
    "        return True\n",
    "    return False\n",
    "  \n",
    "def is_likely_proper(tok):\n",
    "    \"\"\"Returns true if the spacy token is a likely proper name, based on its form.\"\"\"\n",
    "    if len(tok)< 2:\n",
    "        return False\n",
    "    \n",
    "    # If the lemma is titled, just return True\n",
    "    elif tok.lemma_.istitle():\n",
    "        return True\n",
    "       \n",
    "    # Handling cases such as iPad\n",
    "    elif len(tok)>2 and tok.text[0].islower() and tok.text[1].isupper() and tok.text[2:].islower():\n",
    "        return True\n",
    "    \n",
    "    elif (tok.is_upper and tok.text not in spacy_wrapper.CURRENCY_CODES \n",
    "          and tok.text not in spacy_wrapper.NOT_NAMED_ENTITIES):\n",
    "        return True\n",
    "    \n",
    "    # Else, check whether the surface token is titled and is not sentence-initial\n",
    "    elif (tok.i > 0 and tok.is_title and not tok.is_sent_start and tok.nbor(-1).text not in {'\\'', '\"', '‘', '“', '”', '’'} \n",
    "          and not tok.nbor(-1).text.endswith(\".\")):\n",
    "        return True\n",
    "    return False  \n",
    "\n",
    "fullname = FullNameGenerator()\n",
    "fullname_annotator = skweak.heuristics.FunctionAnnotator(\"fullname_detector\", fullname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "clear-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc: Doc) -> Doc:\n",
    "  #apply all labelling functions to doc\n",
    "  #return nationality_annotate(country_annotate(spacy_conll(dutch_loc_annotator(geonames_annotator(crunchbase_annotator(fullname_annotator(spacy_nl(doc))))))))\n",
    "  return spacy_nl(doc)\n",
    "  #return #dutch_loc_annotator(crunchbase_annotator(fullname_annotator(doc)))\n",
    "\n",
    "def process_docs(docs: List[Doc]) -> Doc:\n",
    "  docs = [process_doc(doc) for doc in docs]\n",
    "  hmm = skweak.aggregation.HMM(\"hmm\", [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "  hmm.fit_and_aggregate(docs)\n",
    "  for doc in docs:\n",
    "    doc.ents = doc.spans['hmm']\n",
    "  return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "directed-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Finished E-step with 1 documents\n",
      "Starting iteration 2\n",
      "Finished E-step with 1 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1         -31.4380             +nan\n",
      "         2         -31.4380          +0.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hoi ik ben \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    William\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", 23 jaar en ik woon in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amsterdam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " maar ben nu in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Heerenveen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Ik ga naar de \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Universiteit van Amsterdam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nederland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amerika\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Belgie\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = nlp(\"Hoi ik ben William, 23 jaar en ik woon in Amsterdam maar ben nu in Heerenveen. Ik ga naar de Universiteit van Amsterdam. Nederland, Amerika, Belgie.\")\n",
    "\n",
    "processed = process_docs([sample])\n",
    "  \n",
    "skweak.utils.display_entities(processed[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "clear-habitat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Finished E-step with 119 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1      -21240.6415             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 119 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2      -21240.6415          -0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6441116657422814,\n",
       " 'recall': 0.605071205279611,\n",
       " 'f1': 0.6239813736903377}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "mapper = {\"O\" : 0,\n",
    "          \"PER\" : 1, \n",
    "          \"LOC\" : 2, \n",
    "          \"ORG\" : 3,\n",
    "          \"MISC\" : 4}\n",
    "\n",
    "def ent_iob(tok: Token) -> str:\n",
    "  \"\"\"\n",
    "  returns ent IOB label\n",
    "  \"\"\"\n",
    "  ent_type = \"-\" + tok.ent_type_ if tok.ent_type else \"\"\n",
    "  return tok.ent_iob_ + ent_type\n",
    "\n",
    "def ent_type(tok: Token) -> str:\n",
    "  \"\"\"\n",
    "  returns entity type for given token\n",
    "  \"\"\"\n",
    "  return tok.ent_type_ if tok.ent_type_ else tok.ent_iob_\n",
    "\n",
    "def doc_ents(doc: Doc) -> List[str]:\n",
    "  \"\"\"\n",
    "  returns list of IOB style formatted entities in doc\n",
    "  \"\"\"\n",
    "  return [ent_iob(tok) for tok in doc]\n",
    "  \n",
    "def score_annotator(docs: List[Doc]) -> dict:\n",
    "  \"\"\"\n",
    "  given docs, applies annotator to docs and evaluates against ground truth \n",
    "  \"\"\"\n",
    "  true = [token for doc in docs for token in doc_ents(doc)]\n",
    "  \n",
    "  annotated = process_docs(docs)\n",
    "  pred = [token for doc in annotated for token in doc_ents(doc)]\n",
    "  \n",
    "  #calculate precision, recall, f1 and give it weights to micro average\n",
    "  #OR add all predictions to one humongous list and run functions on that list \n",
    "\n",
    "  labels = [\"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\", \"B-MISC\", \"I-MISC\"]\n",
    "  return {\"precision\" : precision_score(true, pred, labels=labels, average=\"micro\"), \n",
    "          \"recall\" : recall_score(true, pred, labels=labels, average=\"micro\"),\n",
    "          \"f1\" : f1_score(true, pred, labels=labels, average=\"micro\")}\n",
    "\n",
    "\n",
    "sample = test\n",
    "score_annotator(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "elder-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\"a\", \"b\", \"c\", \"d\", \"e\"], dtype=\"object\")\n",
    "b = np.array([\"a\", \"a\", \"c\", \"c\", \"c\"], dtype=\"object\")\n",
    "cm = confusion_matrix(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "tracked-franchise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4000000000000001"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset, ClassLabel, Sequence\n",
    "doc = nlp(sample.text)\n",
    "\n",
    "def name_detector_fn(doc):\n",
    "    names = [\"David\", \"Cameron\", \"William\", \"Bos\"]\n",
    "    for token in doc: \n",
    "        if token.text in names:\n",
    "            yield token.i, token.i+1, \"PER\"\n",
    "\n",
    "name_detector = skweak.heuristics.FunctionAnnotator(\"name_detector\", name_detector_fn)\n",
    "\n",
    "geonames = skweak.gazetteers.extract_json_data(\"data/geonames.json\", spacy_model=\"en_core_web_sm\")\n",
    "geonames_annotator = skweak.gazetteers.GazetteerAnnotator(\"locations\", geonames)\n",
    "\n",
    "\n",
    "#hmm = skweak.aggregation.HMM(\"hmm\", [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "#hmm.fit_and_aggregate([doc])\n",
    "#spans = doc.spans[\"hmm\"]\n",
    "#doc.ents = spans\n",
    "\n",
    "#for span in doc.ents:\n",
    "#  print(span, span.label, span.start, span.end, span.doc[span.start : span.end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(geonames_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, ClassLabel, Sequence\n",
    "\n",
    "def relabel(ent_label: str) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 label of Spacy labelled entity\n",
    "  \"\"\"\n",
    "  mappings = {\"PERSON\":\"PER\", \"COMPANY\":\"ORG\", \"GPE\":\"LOC\", 'EVENT':\"MISC\", 'FAC':\"MISC\", 'LANGUAGE':\"MISC\", 'LAW':\"MISC\", 'NORP':\"MISC\", 'PRODUCT':\"MISC\",'WORK_OF_ART':\"MISC\", \"MISC\":\"MISC\", \"PER\":\"PER\", \"ORG\":\"ORG\", \"LOC\":\"LOC\"}    \n",
    "  exclude = {\"CARDINAL\", \"ORDINAL\", \"DATE\", \"PERCENT\", \"QUANTITY\", \"TIME\", \"MONEY\"}\n",
    "\n",
    "  return mappings[ent_label] if ent_label != \"\" and ent_label not in exclude else None\n",
    "\n",
    "def convert_ent(token) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 IOB style entity label of Spacy token\n",
    "  \"\"\"\n",
    "  return token.ent_iob_ + \"-\" + relabel(token.ent_type_) if relabel(token.ent_type_) else token.ent_iob_\n",
    "\n",
    "def process_spacy(docs: list):\n",
    "  store = []\n",
    "  tokens = []\n",
    "  ids = []\n",
    "\n",
    "  c = 0\n",
    "  classlabels = ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'])\n",
    "  for doc in docs:\n",
    "    ents = [classlabels.str2int(convert_ent(tok)) for tok in doc]\n",
    "    toks = [token.text for token in doc]\n",
    "    store.append(ents)\n",
    "    tokens.append(toks)\n",
    "    ids.append(str(c))\n",
    "    c += 1 \n",
    "    \n",
    "  d = {\"ids\" : ids,\n",
    "       \"ner_tags\" : store,\n",
    "       \"tokens\" : tokens}\n",
    "\n",
    "  class_sequence = Sequence(feature =  classlabels, id = None)\n",
    "  ds = Dataset.from_dict(d)\n",
    "  ds.features[\"ner_tags\"] = class_sequence\n",
    "  return ds\n",
    "\n",
    "ds = process_spacy([doc])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b361823-c157-4bf7-ae16-4d3105914fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# We retrieve the texts from Wablieft tarfile\n",
    "texts = [] \n",
    "archive_file = tarfile.open(\"data/plain.tar.xz\")\n",
    "for archive_member in archive_file.getnames():\n",
    "    if archive_member.endswith(\".txt\"):\n",
    "        #weird encoding because of Turkish letters\n",
    "        text = archive_file.extractfile(archive_member).read().decode(\"cp850\")\n",
    "        texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc8d9ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/geonames.json\n",
      "Populating trie for class GPE (number: 15205)\n"
     ]
    }
   ],
   "source": [
    "def name_detector_fn(doc):\n",
    "    names = [\"David\", \"Cameron\", \"William\", \"Bos\"]\n",
    "    for token in doc: \n",
    "        if token.text in names:\n",
    "            yield token.i, token.i+1, \"PER\"\n",
    "\n",
    "name_detector = skweak.heuristics.FunctionAnnotator(\"name_detector\", name_detector_fn)\n",
    "\n",
    "def address_detector_fn(doc):\n",
    "    for token in doc: \n",
    "        if token.text[0].isupper() and doc[token.i+1].is_digit:\n",
    "            yield token.i, token.i+2, \"LOCATION\"\n",
    "\n",
    "address_detector = skweak.heuristics.FunctionAnnotator(\"address_detector\", address_detector_fn)\n",
    "\n",
    "def company_detector_fn(doc):\n",
    "    companies = [\"Microsoft\", \"Apple\", \"Gemeente Amsterdam\", \"Universiteit van Amsterdam\", \"UvA\"]\n",
    "    for token in doc:\n",
    "        if token.text in companies:\n",
    "            yield token.i, token.i+1, \"ORG\"\n",
    "\n",
    "company_detector = skweak.heuristics.FunctionAnnotator(\"company_detector\", company_detector_fn)\n",
    "\n",
    "names = skweak.gazetteers.extract_json_data(\"data/geonames.json\", spacy_model=\"en_core_web_sm\")\n",
    "name_annotator = skweak.gazetteers.GazetteerAnnotator(\"locations\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "naval-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(address_detector) == skweak.heuristics.FunctionAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = list(name_detector.pipe(name_annotator.pipe(docs[:25])))\n",
    "hmm = skweak.aggregation.HMM(\"hmm\", [\"PERSON\", \"LOCATION\", \"ORG\", \"GPE\"])\n",
    "\n",
    "#skweak.utils.display_entities(processed[7], \"locations\")\n",
    "hmm.fit_and_aggregate(processed)\n",
    "skweak.utils.display_entities(processed[7], \"hmm\", add_tooltip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb882579-f2b5-42b7-949b-acd14f6bc932",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in processed:\n",
    "    if \"hmm\" in doc.spans.keys():\n",
    "        doc.ents = doc.spans[\"hmm\"]\n",
    "    else: \n",
    "        doc.ents = ()\n",
    "\n",
    "docs = docs[:100]\n",
    "skweak.utils.docbin_writer(docs, \"data/wablieft.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e7409-4b44-4ca5-98c6-c1acc49e1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b9864-0173-49fa-b912-601574d3e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy train config.cfg --paths.train data/wablieft.spacy --paths.dev data/wablieft.spacy --initialize.vectors nl_core_news_md --output output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d60f36-2ee5-4ace-8004-cef7f35e33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load(\"output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4866f2-3b2b-4bea-914d-39e74acb1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = processed[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe064c-6b76-40e6-b25e-3b177976a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "skweak.utils.display_entities(model(\"Hoi ik ben William en ik woon in New York. Ik kom uit Flanders en ben geboren in Antwerpen.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990abbc5-e408-4044-8038-bacb93a0f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lison2020 import annotations\n",
    "from spacy.tokens import DocBin\n",
    "test = DocBin().from_disk(\"data/ned_testb.spacy\")\n",
    "sys.path.insert(0, './lison2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457143e0-71a1-42ea-9665-1b692bc93f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = annotations.FullAnnotator().add_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066d396-ada1-421c-9fef-35eb03d8e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598172f-9de8-483f-9a07-b35e0f721559",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.spacy_benchmark(\"data/ned_testb.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7940a0-c9f8-4149-a8df-f217bd693862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
