{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d65f70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "import skweak\n",
    "import evaluation as evn\n",
    "import itertools\n",
    "from skweak.spacy import ModelAnnotator\n",
    "from spacy.tokens import Doc\n",
    "from typing import List, Set, Dict, Tuple, Optional\n",
    "nlp = spacy.load(\"nl_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sophisticated-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODAY I'LL TRAIN SOME SPACY MODELS - LET'S SEE... SONAR-1 AND CONLL-2002? \n",
    "\n",
    "#I ALSO HAVE TO MAKE A PRETRAINED CONLL-2002 XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-masters",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "structured-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = evn.to_spacy(\"data/ned_train.spacy\")\n",
    "val = evn.to_spacy(\"data/ned_testa.spacy\")\n",
    "sample = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "designed-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(ent_label: str) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 label of Spacy labelled entity\n",
    "  \"\"\"\n",
    "  mappings = {\"PERSON\":\"PER\", \"COMPANY\":\"ORG\", \"GPE\":\"LOC\", 'EVENT':\"MISC\", 'FAC':\"MISC\", 'LANGUAGE':\"MISC\", 'LAW':\"MISC\", 'NORP':\"MISC\", 'PRODUCT':\"MISC\",'WORK_OF_ART':\"MISC\", \"MISC\":\"MISC\", \"PER\":\"PER\", \"ORG\":\"ORG\", \"LOC\":\"LOC\"}    \n",
    "  exclude = {\"CARDINAL\", \"ORDINAL\", \"DATE\", \"PERCENT\", \"QUANTITY\", \"TIME\", \"MONEY\"}\n",
    "\n",
    "  return mappings[ent_label] if ent_label != \"\" and ent_label not in exclude else \"O\"\n",
    "\n",
    "class RelabelledModelAnnotator(ModelAnnotator):\n",
    "  def __init__(self, name: str, model_path: str):\n",
    "    super(RelabelledModelAnnotator, self).__init__(name, model_path)\n",
    "    \n",
    "  def find_spans(self, doc):\n",
    "    # Create a new document (to avoid conflicting annotations)\n",
    "    doc2 = self.create_new_doc(doc)\n",
    "    # And run the model\n",
    "    for _, proc in self.model.pipeline:\n",
    "        doc2 = proc(doc2)\n",
    "    # Add the annotation\n",
    "    for ent in doc2.ents:\n",
    "        #just put relabel function here\n",
    "        yield ent.start, ent.end, relabel(ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "british-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model annotators\n",
    "spacy_nl = RelabelledModelAnnotator(\"spacy\", \"nl_core_news_md\")\n",
    "spacy_conll = RelabelledModelAnnotator(\"conll\", \"models/conll2002_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaged-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/geonames.json\n",
      "Populating trie for class LOC (number: 15205)\n",
      "Extracting data from data/nl.json\n",
      "Populating trie for class LOC (number: 22819)\n",
      "Extracting data from data/crunchbase_alt.json\n",
      "Populating trie for class PER (number: 1062669)\n",
      "Populating trie for class ORG (number: 789205)\n"
     ]
    }
   ],
   "source": [
    "#gazetteers \n",
    "geonames = skweak.gazetteers.extract_json_data(\"data/geonames.json\")\n",
    "geonames_annotator = skweak.gazetteers.GazetteerAnnotator(\"geonames\", geonames)\n",
    "\n",
    "nederlocs = skweak.gazetteers.extract_json_data(\"data/nl.json\")\n",
    "dutch_loc_annotator = skweak.gazetteers.GazetteerAnnotator(\"nederlocs\", nederlocs)\n",
    "\n",
    "crunchbase = skweak.gazetteers.extract_json_data(\"data/crunchbase_alt.json\")\n",
    "crunchbase_annotator = skweak.gazetteers.GazetteerAnnotator(\"crunchbase\", crunchbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "material-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions \n",
    "import json\n",
    "import spacy_wrapper\n",
    "\n",
    "NAME_PREFIXES = {\"-\", \"von\", \"van\", \"de\", \"di\", \"le\", \"la\", \"het\", \"'t'\", \"dem\", \"der\", \"den\", \"d'\", \"ter\"}\n",
    "\n",
    "class SpanGenerator:\n",
    "    \"\"\"Generate spans that satisfy a token-level constratint. From Lison et al. 2020\"\"\"\n",
    "    \n",
    "    def __init__(self, constraint, label=\"ENT\", exceptions=(\"'s\", \"-\")):\n",
    "        \"\"\"annotation with a constraint (on spacy tokens). Exceptions are sets of tokens that are allowed\n",
    "        to violate the constraint inside the span\"\"\"\n",
    "        \n",
    "        self.constraint = constraint\n",
    "        self.label = label\n",
    "        self.exceptions = set(exceptions)\n",
    "        \n",
    "    def __call__(self, spacy_doc):    \n",
    "\n",
    "        i = 0\n",
    "        while i < len(spacy_doc):\n",
    "            tok = spacy_doc[i]\n",
    "                # We search for the longest span that satisfy the constraint\n",
    "            if self.constraint(tok):\n",
    "                j = i+1\n",
    "                while True:\n",
    "                    if j < len(spacy_doc) and self.constraint(spacy_doc[j]):\n",
    "                        j += 1\n",
    "                    # We relax the constraint a bit to allow genitive and dashes\n",
    "                    elif j < (len(spacy_doc)-1) and spacy_doc[j].text in self.exceptions and self.constraint(spacy_doc[j+1]):\n",
    "                        j += 2\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # To avoid too many FPs, we only keep entities with at least 3 characters (excluding punctuation)\n",
    "                if len(spacy_doc[i:j].text.rstrip(\".\")) > 2:\n",
    "                    yield i, j, self.label\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "class FullNameGenerator:\n",
    "    \"\"\"Search for occurrences of full person names (first name followed by at least one title token). From Lison et al. 2020\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        fd = open(\"data/first_names.json\")\n",
    "        self.first_names = set(json.load(fd))\n",
    "        fd.close()\n",
    "        self.suggest_generator = SpanGenerator(lambda x: is_likely_proper(x), \n",
    "                                               exceptions=NAME_PREFIXES)\n",
    "        \n",
    "    def __call__(self, spacy_doc):\n",
    "        for start, end, _ in self.suggest_generator(spacy_doc):  \n",
    "            # We assume full names are between 2 and 4 tokens\n",
    "            if (end-start) < 2 or (end-start) > 5:\n",
    "                continue\n",
    "                \n",
    "            elif (spacy_doc[start].text in self.first_names and spacy_doc[end-1].is_alpha \n",
    "                  and spacy_doc[end-1].is_title): \n",
    "                yield start, end, \"PER\"\n",
    "\n",
    "def in_compound(tok):\n",
    "    \"\"\"Returns true if the spacy token is part of a compound phrase\"\"\"\n",
    "    if tok.dep_==\"compound\":\n",
    "        return True\n",
    "    elif tok.i > 0 and tok.nbor(-1).dep_==\"compound\":\n",
    "        return True\n",
    "    return False\n",
    "  \n",
    "def is_likely_proper(tok):\n",
    "    \"\"\"Returns true if the spacy token is a likely proper name, based on its form.\"\"\"\n",
    "    if len(tok)< 2:\n",
    "        return False\n",
    "    \n",
    "    # If the lemma is titled, just return True\n",
    "    elif tok.lemma_.istitle():\n",
    "        return True\n",
    "       \n",
    "    # Handling cases such as iPad\n",
    "    elif len(tok)>2 and tok.text[0].islower() and tok.text[1].isupper() and tok.text[2:].islower():\n",
    "        return True\n",
    "    \n",
    "    elif (tok.is_upper and tok.text not in spacy_wrapper.CURRENCY_CODES \n",
    "          and tok.text not in spacy_wrapper.NOT_NAMED_ENTITIES):\n",
    "        return True\n",
    "    \n",
    "    # Else, check whether the surface token is titled and is not sentence-initial\n",
    "    elif (tok.i > 0 and tok.is_title and not tok.is_sent_start and tok.nbor(-1).text not in {'\\'', '\"', '‘', '“', '”', '’'} \n",
    "          and not tok.nbor(-1).text.endswith(\".\")):\n",
    "        return True\n",
    "    return False  \n",
    "\n",
    "fullname = FullNameGenerator()\n",
    "fullname_annotator = skweak.heuristics.FunctionAnnotator(\"fullname_detector\", fullname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "important-infection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testcell\n",
    "test = nlp(val[0].text)\n",
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "clear-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc: Doc) -> Doc:\n",
    "  #apply all labelling functions to doc\n",
    "  return spacy_conll(dutch_loc_annotator(geonames_annotator(crunchbase_annotator(fullname_annotator(spacy_nl(doc))))))\n",
    "\n",
    "def process_docs(docs: List[Doc]) -> Doc:\n",
    "  hmm = skweak.aggregation.HMM(\"hmm\", [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "  hmm.fit_and_aggregate(docs)\n",
    "  \n",
    "  return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "directed-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Finished E-step with 1 documents\n",
      "Starting iteration 2\n",
      "Finished E-step with 1 documents\n",
      "William 4317129024397789502\n",
      "Amsterdam 385\n",
      "Heerenveen 385\n",
      "Universiteit van Amsterdam 383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1         -24.9439             +nan\n",
      "         2         -24.9439          -0.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hoi ik ben \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    William\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", 23 jaar en ik woon in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amsterdam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " maar ben nu in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Heerenveen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Ik ga naar de \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Universiteit van Amsterdam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = nlp(\"Hoi ik ben William, 23 jaar en ik woon in Amsterdam maar ben nu in Heerenveen. Ik ga naar de Universiteit van Amsterdam.\")\n",
    "for tok in test:\n",
    "  if tok == None:\n",
    "    print(\"none\")\n",
    "\n",
    "# for tok in test:\n",
    "#   print(tok)\n",
    "#   spacy_nl(nlp(tok.text))\n",
    "#processed = spacy_conll(dutch_loc_annotator(geonames_annotator(crunchbase_annotator(fullname_annotator(spacy_nl(test))))))\n",
    "processed = spacy_nl(sample)\n",
    "hmm = skweak.aggregation.HMM(\"hmm\", [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "hmm.fit_and_aggregate([processed])\n",
    "spans = processed.spans[\"hmm\"]\n",
    "\n",
    "processed.ents = spans\n",
    "for span in processed.ents:\n",
    "  print(span, span.label)\n",
    "  \n",
    "skweak.utils.display_entities(processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "christian-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = val[:5]\n",
    "sample = [process_doc(doc) for doc in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aquatic-calendar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Finished E-step with 5 documents\n",
      "Starting iteration 2\n",
      "Finished E-step with 5 documents\n",
      "Starting iteration 3\n",
      "Finished E-step with 5 documents\n",
      "Starting iteration 4\n",
      "Finished E-step with 5 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1       -1479.7843             +nan\n",
      "         2       -1426.6107         +53.1735\n",
      "         3       -1416.5156         +10.0952\n",
      "         4       -1413.1060          +3.4096\n"
     ]
    }
   ],
   "source": [
    "sample = process_docs(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "catholic-guatemala",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Düsseldorf, Duitsers, Eisenach, De Morgen, DPA, Reuters, Duitsland, Düsseldorf, Binnenlandse Zaken, Otto Schily, SPD, Duitsers, Duitsland, Noord-Duitse, stad Stralsund, joden, Russische, Stralsund, Eisenach, DDR, Frank Schlömer, Stralsund, Eisenach, Togo, Soedan, Sieg Heil, Duitse, Düsseldorf, Eisenach, Berlijn, Duitse, Oost-Duitse, Eisenach, Afrikanen]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[3].spans[\"hmm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset, ClassLabel, Sequence\n",
    "doc = nlp(sample.text)\n",
    "\n",
    "def name_detector_fn(doc):\n",
    "    names = [\"David\", \"Cameron\", \"William\", \"Bos\"]\n",
    "    for token in doc: \n",
    "        if token.text in names:\n",
    "            yield token.i, token.i+1, \"PER\"\n",
    "\n",
    "name_detector = skweak.heuristics.FunctionAnnotator(\"name_detector\", name_detector_fn)\n",
    "\n",
    "geonames = skweak.gazetteers.extract_json_data(\"data/geonames.json\", spacy_model=\"en_core_web_sm\")\n",
    "geonames_annotator = skweak.gazetteers.GazetteerAnnotator(\"locations\", geonames)\n",
    "\n",
    "\n",
    "#hmm = skweak.aggregation.HMM(\"hmm\", [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "#hmm.fit_and_aggregate([doc])\n",
    "#spans = doc.spans[\"hmm\"]\n",
    "#doc.ents = spans\n",
    "\n",
    "#for span in doc.ents:\n",
    "#  print(span, span.label, span.start, span.end, span.doc[span.start : span.end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(geonames_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, ClassLabel, Sequence\n",
    "\n",
    "def relabel(ent_label: str) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 label of Spacy labelled entity\n",
    "  \"\"\"\n",
    "  mappings = {\"PERSON\":\"PER\", \"COMPANY\":\"ORG\", \"GPE\":\"LOC\", 'EVENT':\"MISC\", 'FAC':\"MISC\", 'LANGUAGE':\"MISC\", 'LAW':\"MISC\", 'NORP':\"MISC\", 'PRODUCT':\"MISC\",'WORK_OF_ART':\"MISC\", \"MISC\":\"MISC\", \"PER\":\"PER\", \"ORG\":\"ORG\", \"LOC\":\"LOC\"}    \n",
    "  exclude = {\"CARDINAL\", \"ORDINAL\", \"DATE\", \"PERCENT\", \"QUANTITY\", \"TIME\", \"MONEY\"}\n",
    "\n",
    "  return mappings[ent_label] if ent_label != \"\" and ent_label not in exclude else None\n",
    "\n",
    "def convert_ent(token) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 IOB style entity label of Spacy token\n",
    "  \"\"\"\n",
    "  return token.ent_iob_ + \"-\" + relabel(token.ent_type_) if relabel(token.ent_type_) else token.ent_iob_\n",
    "\n",
    "def process_spacy(docs: list):\n",
    "  store = []\n",
    "  tokens = []\n",
    "  ids = []\n",
    "\n",
    "  c = 0\n",
    "  classlabels = ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'])\n",
    "  for doc in docs:\n",
    "    ents = [classlabels.str2int(convert_ent(tok)) for tok in doc]\n",
    "    toks = [token.text for token in doc]\n",
    "    store.append(ents)\n",
    "    tokens.append(toks)\n",
    "    ids.append(str(c))\n",
    "    c += 1 \n",
    "    \n",
    "  d = {\"ids\" : ids,\n",
    "       \"ner_tags\" : store,\n",
    "       \"tokens\" : tokens}\n",
    "\n",
    "  class_sequence = Sequence(feature =  classlabels, id = None)\n",
    "  ds = Dataset.from_dict(d)\n",
    "  ds.features[\"ner_tags\"] = class_sequence\n",
    "  return ds\n",
    "\n",
    "ds = process_spacy([doc])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b361823-c157-4bf7-ae16-4d3105914fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# We retrieve the texts from Wablieft tarfile\n",
    "texts = [] \n",
    "archive_file = tarfile.open(\"data/plain.tar.xz\")\n",
    "for archive_member in archive_file.getnames():\n",
    "    if archive_member.endswith(\".txt\"):\n",
    "        #weird encoding because of Turkish letters\n",
    "        text = archive_file.extractfile(archive_member).read().decode(\"cp850\")\n",
    "        texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc8d9ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/geonames.json\n",
      "Populating trie for class GPE (number: 15205)\n"
     ]
    }
   ],
   "source": [
    "def name_detector_fn(doc):\n",
    "    names = [\"David\", \"Cameron\", \"William\", \"Bos\"]\n",
    "    for token in doc: \n",
    "        if token.text in names:\n",
    "            yield token.i, token.i+1, \"PER\"\n",
    "\n",
    "name_detector = skweak.heuristics.FunctionAnnotator(\"name_detector\", name_detector_fn)\n",
    "\n",
    "def address_detector_fn(doc):\n",
    "    for token in doc: \n",
    "        if token.text[0].isupper() and doc[token.i+1].is_digit:\n",
    "            yield token.i, token.i+2, \"LOCATION\"\n",
    "\n",
    "address_detector = skweak.heuristics.FunctionAnnotator(\"address_detector\", address_detector_fn)\n",
    "\n",
    "def company_detector_fn(doc):\n",
    "    companies = [\"Microsoft\", \"Apple\", \"Gemeente Amsterdam\", \"Universiteit van Amsterdam\", \"UvA\"]\n",
    "    for token in doc:\n",
    "        if token.text in companies:\n",
    "            yield token.i, token.i+1, \"ORG\"\n",
    "\n",
    "company_detector = skweak.heuristics.FunctionAnnotator(\"company_detector\", company_detector_fn)\n",
    "\n",
    "names = skweak.gazetteers.extract_json_data(\"data/geonames.json\", spacy_model=\"en_core_web_sm\")\n",
    "name_annotator = skweak.gazetteers.GazetteerAnnotator(\"locations\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "naval-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(address_detector) == skweak.heuristics.FunctionAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = list(name_detector.pipe(name_annotator.pipe(docs[:25])))\n",
    "hmm = skweak.aggregation.HMM(\"hmm\", [\"PERSON\", \"LOCATION\", \"ORG\", \"GPE\"])\n",
    "\n",
    "#skweak.utils.display_entities(processed[7], \"locations\")\n",
    "hmm.fit_and_aggregate(processed)\n",
    "skweak.utils.display_entities(processed[7], \"hmm\", add_tooltip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb882579-f2b5-42b7-949b-acd14f6bc932",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in processed:\n",
    "    if \"hmm\" in doc.spans.keys():\n",
    "        doc.ents = doc.spans[\"hmm\"]\n",
    "    else: \n",
    "        doc.ents = ()\n",
    "\n",
    "docs = docs[:100]\n",
    "skweak.utils.docbin_writer(docs, \"data/wablieft.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e7409-4b44-4ca5-98c6-c1acc49e1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b9864-0173-49fa-b912-601574d3e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy train config.cfg --paths.train data/wablieft.spacy --paths.dev data/wablieft.spacy --initialize.vectors nl_core_news_md --output output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d60f36-2ee5-4ace-8004-cef7f35e33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load(\"output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4866f2-3b2b-4bea-914d-39e74acb1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = processed[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe064c-6b76-40e6-b25e-3b177976a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "skweak.utils.display_entities(model(\"Hoi ik ben William en ik woon in New York. Ik kom uit Flanders en ben geboren in Antwerpen.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990abbc5-e408-4044-8038-bacb93a0f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lison2020 import annotations\n",
    "from spacy.tokens import DocBin\n",
    "test = DocBin().from_disk(\"data/ned_testb.spacy\")\n",
    "sys.path.insert(0, './lison2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457143e0-71a1-42ea-9665-1b692bc93f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = annotations.FullAnnotator().add_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066d396-ada1-421c-9fef-35eb03d8e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598172f-9de8-483f-9a07-b35e0f721559",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.spacy_benchmark(\"data/ned_testb.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7940a0-c9f8-4149-a8df-f217bd693862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
