{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65f70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "import skweak\n",
    "import evaluation as evn\n",
    "import itertools\n",
    "from skweak.spacy import ModelAnnotator\n",
    "nlp = spacy.load(\"nl_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sophisticated-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODAY I'LL TRAIN SOME SPACY MODELS - LET'S SEE... SONAR-1 AND CONLL-2002? \n",
    "\n",
    "#I ALSO HAVE TO MAKE A PRETRAINED CONLL-2002 XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "super-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def scream(strings: list):\n",
    "  return [string.upper() for string in strings]\n",
    "\n",
    "def double(strings: list):\n",
    "  return [string + \" \" + string for string in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "endless-princess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['HOI', 'HOE', 'GAAT', 'HET']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs = [scream, double]\n",
    "docs = [\"hoi\", \"hoe\", \"gaat\", \"het\"]\n",
    "streams = itertools.tee(docs, len(funcs)+1)\n",
    "[func(string) for func, string in zip(funcs, streams[:1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "structured-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = evn.to_spacy(\"data/ned_train.spacy\")\n",
    "sample = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "designed-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(ent_label: str) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 label of Spacy labelled entity\n",
    "  \"\"\"\n",
    "  mappings = {\"PERSON\":\"PER\", \"COMPANY\":\"ORG\", \"GPE\":\"LOC\", 'EVENT':\"MISC\", 'FAC':\"MISC\", 'LANGUAGE':\"MISC\", 'LAW':\"MISC\", 'NORP':\"MISC\", 'PRODUCT':\"MISC\",'WORK_OF_ART':\"MISC\", \"MISC\":\"MISC\", \"PER\":\"PER\", \"ORG\":\"ORG\", \"LOC\":\"LOC\"}    \n",
    "  exclude = {\"CARDINAL\", \"ORDINAL\", \"DATE\", \"PERCENT\", \"QUANTITY\", \"TIME\", \"MONEY\"}\n",
    "\n",
    "  return mappings[ent_label] if ent_label != \"\" and ent_label not in exclude else None\n",
    "\n",
    "class RelabelledModelAnnotator(ModelAnnotator):\n",
    "  def __init__(self, name: str, model_path: str):\n",
    "    super(RelabelledModelAnnotator, self).__init__(name, model_path)\n",
    "    \n",
    "  def find_spans(self, doc):\n",
    "    # Create a new document (to avoid conflicting annotations)\n",
    "    doc2 = self.create_new_doc(doc)\n",
    "    # And run the model\n",
    "    for _, proc in self.model.pipeline:\n",
    "        doc2 = proc(doc2)\n",
    "    # Add the annotation\n",
    "    for ent in doc2.ents:\n",
    "        #just put relabel function here\n",
    "        yield ent.start, ent.end, relabel(ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "british-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model annotators\n",
    "spacy_nl = RelabelledModelAnnotator(\"spacy\", \"nl_core_news_md\")\n",
    "spacy_conll = RelabelledModelAnnotator(\"conll\", \"models/conll2002_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "engaged-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/geonames.json\n",
      "Populating trie for class LOC (number: 15205)\n",
      "Extracting data from data/nl.json\n",
      "Populating trie for class LOC (number: 22819)\n",
      "Extracting data from data/crunchbase_alt.json\n",
      "Populating trie for class PER (number: 1062669)\n",
      "Populating trie for class ORG (number: 789205)\n"
     ]
    }
   ],
   "source": [
    "#gazetteers \n",
    "geonames = skweak.gazetteers.extract_json_data(\"data/geonames.json\")\n",
    "geonames_annotator = skweak.gazetteers.GazetteerAnnotator(\"geonames\", geonames)\n",
    "\n",
    "nederlocs = skweak.gazetteers.extract_json_data(\"data/nl.json\")\n",
    "dutch_loc_annotator = skweak.gazetteers.GazetteerAnnotator(\"nederlocs\", nederlocs)\n",
    "\n",
    "crunchbase = skweak.gazetteers.extract_json_data(\"data/crunchbase_alt.json\")\n",
    "crunchbase_annotator = skweak.gazetteers.GazetteerAnnotator(\"crunchbase\", crunchbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "material-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions \n",
    "import json\n",
    "import spacy_wrapper\n",
    "\n",
    "NAME_PREFIXES = {\"-\", \"von\", \"van\", \"de\", \"di\", \"le\", \"la\", \"het\", \"'t'\", \"dem\", \"der\", \"den\", \"d'\", \"ter\"}\n",
    "\n",
    "class SpanGenerator:\n",
    "    \"\"\"Generate spans that satisfy a token-level constratint. From Lison et al. 2020\"\"\"\n",
    "    \n",
    "    def __init__(self, constraint, label=\"ENT\", exceptions=(\"'s\", \"-\")):\n",
    "        \"\"\"annotation with a constraint (on spacy tokens). Exceptions are sets of tokens that are allowed\n",
    "        to violate the constraint inside the span\"\"\"\n",
    "        \n",
    "        self.constraint = constraint\n",
    "        self.label = label\n",
    "        self.exceptions = set(exceptions)\n",
    "        \n",
    "    def __call__(self, spacy_doc):    \n",
    "\n",
    "        i = 0\n",
    "        while i < len(spacy_doc):\n",
    "            tok = spacy_doc[i]\n",
    "                # We search for the longest span that satisfy the constraint\n",
    "            if self.constraint(tok):\n",
    "                j = i+1\n",
    "                while True:\n",
    "                    if j < len(spacy_doc) and self.constraint(spacy_doc[j]):\n",
    "                        j += 1\n",
    "                    # We relax the constraint a bit to allow genitive and dashes\n",
    "                    elif j < (len(spacy_doc)-1) and spacy_doc[j].text in self.exceptions and self.constraint(spacy_doc[j+1]):\n",
    "                        j += 2\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # To avoid too many FPs, we only keep entities with at least 3 characters (excluding punctuation)\n",
    "                if len(spacy_doc[i:j].text.rstrip(\".\")) > 2:\n",
    "                    yield i, j, self.label\n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "class FullNameGenerator:\n",
    "    \"\"\"Search for occurrences of full person names (first name followed by at least one title token). From Lison et al. 2020\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        fd = open(\"data/first_names.json\")\n",
    "        self.first_names = set(json.load(fd))\n",
    "        fd.close()\n",
    "        self.suggest_generator = SpanGenerator(lambda x: is_likely_proper(x), \n",
    "                                               exceptions=NAME_PREFIXES)\n",
    "        \n",
    "    def __call__(self, spacy_doc):\n",
    "        for start, end, _ in self.suggest_generator(spacy_doc):  \n",
    "            # We assume full names are between 2 and 4 tokens\n",
    "            if (end-start) < 2 or (end-start) > 5:\n",
    "                continue\n",
    "                \n",
    "            elif (spacy_doc[start].text in self.first_names and spacy_doc[end-1].is_alpha \n",
    "                  and spacy_doc[end-1].is_title): \n",
    "                yield start, end, \"PER\"\n",
    "\n",
    "def in_compound(tok):\n",
    "    \"\"\"Returns true if the spacy token is part of a compound phrase\"\"\"\n",
    "    if tok.dep_==\"compound\":\n",
    "        return True\n",
    "    elif tok.i > 0 and tok.nbor(-1).dep_==\"compound\":\n",
    "        return True\n",
    "    return False\n",
    "  \n",
    "def is_likely_proper(tok):\n",
    "    \"\"\"Returns true if the spacy token is a likely proper name, based on its form.\"\"\"\n",
    "    if len(tok)< 2:\n",
    "        return False\n",
    "    \n",
    "    # If the lemma is titled, just return True\n",
    "    elif tok.lemma_.istitle():\n",
    "        return True\n",
    "       \n",
    "    # Handling cases such as iPad\n",
    "    elif len(tok)>2 and tok.text[0].islower() and tok.text[1].isupper() and tok.text[2:].islower():\n",
    "        return True\n",
    "    \n",
    "    elif (tok.is_upper and tok.text not in spacy_wrapper.CURRENCY_CODES \n",
    "          and tok.text not in spacy_wrapper.NOT_NAMED_ENTITIES):\n",
    "        return True\n",
    "    \n",
    "    # Else, check whether the surface token is titled and is not sentence-initial\n",
    "    elif (tok.i > 0 and tok.is_title and not tok.is_sent_start and tok.nbor(-1).text not in {'\\'', '\"', '‘', '“', '”', '’'} \n",
    "          and not tok.nbor(-1).text.endswith(\".\")):\n",
    "        return True\n",
    "    return False  \n",
    "\n",
    "fullname = FullNameGenerator()\n",
    "fullname_annotator = skweak.heuristics.FunctionAnnotator(\"fullname_detector\", fullname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "directed-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Finished E-step with 1 documents\n",
      "Starting iteration 2\n",
      "Finished E-step with 1 documents\n",
      "Starting iteration 3\n",
      "Finished E-step with 1 documents\n",
      "Starting iteration 4\n",
      "Finished E-step with 1 documents\n",
      "William Bos 4317129024397789502\n",
      "Nederland 385\n",
      "Drijberse Veld 385\n",
      "Zwormertorenbrug 385\n",
      "Gemeente Amsterdam 385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1         -59.0496             +nan\n",
      "         2         -54.4113          +4.6383\n",
      "         3         -52.0182          +2.3932\n",
      "         4         -51.9985          +0.0197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Ik, \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    William Bos\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", liep in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nederland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " over het \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Drijberse Veld\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " wat te vinden is in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zwormertorenbrug\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " op weg naar mijn werk bij \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Gemeente Amsterdam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testcell\n",
    "test = nlp(\"Ik, William Bos, liep in Nederland over het Drijberse Veld wat te vinden is in Zwormertorenbrug op weg naar mijn werk bij Gemeente Amsterdam\")\n",
    "\n",
    "processed = spacy_conll(dutch_loc_annotator(geonames_annotator(crunchbase_annotator(fullname_annotator(spacy_nl(test))))))\n",
    "hmm = skweak.aggregation.HMM(\"hmm\", [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "hmm.fit_and_aggregate([processed])\n",
    "spans = processed.spans[\"hmm\"]\n",
    "\n",
    "processed.ents = spans\n",
    "for span in processed.ents:\n",
    "  print(span, span.label)\n",
    "  \n",
    "skweak.utils.display_entities(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sized-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/geonames.json\n",
      "Populating trie for class LOC (number: 15205)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "De tekst van het arrest is nog niet schriftelijk beschikbaar maar het bericht werd alvast bekendgemaakt door een communicatiebureau dat Floralux inhuurde . In '81 regulariseert de toenmalige Vlaamse regering de toestand met een BPA dat het bedrijf op eigen kosten heeft laten opstellen . publicatie Vandaag is Floralux dus met alle vergunningen in orde , maar het BPA waarmee die konden verkregen worden , was omstreden omdat zaakvoerster Christiane Vandenbussche haar schepenambt van ... In eerste aanleg werd Vandenbussche begin de jaren '90 veroordeeld wegens belangenvermenging maar later vrijgesproken door het hof van beroep in Gent . SP zit moeilijk -- ( Eric , sd ) Derycke wil BPA goedkeuren op voorwaarde dat BPA ' De Hoogte ' beperkt wordt aanvaard ( burgemeester van Moorslede Walter Ghekiere zal 'n voorstel doen ) . Onvoldoende om een zware straf uit te spreken , luidt het . Dit hof verbindt nu geen straf aan de schuld die ze vaststelt . Die groeit al snel uit tot een heus tuincentrum . pagina Er worden veel meer zaken verkocht dan je in een plantenkwekerij zou verwachten . Dat blijkt overduidelijk uit een fax die haar advocaat destijds aan haar schreef en die bij een huiszoeking in beslag genomen werd . Het dossier gaat terug tot eind de jaren ' 80 . Sybille Decoo Ruimtelijke Ordening zou hebben misbruikt om het BPA te verkrijgen . Het enige bewijs dat overbleef om Vandenbussche te veroordelen voor ' belangenneming ' was haar aanwezigheid op het schepencollege toen het dossier-Floralux aan bod kwam . Eind '98 maakt de Vlaamse regering deze regularisatie definitief met een gewestplanwijziging . Schuld maar geen boete . \" Zelf deed hij dit af als \" larie en apekool van kwatongen \" . De Morgen Vijgen na Pasen , zo lijkt het hof van beroep te hebben gedacht . Antwerpen / Brussel Plaatselijk wordt vermoed dat Floralux de kiescampagnes van Luc Martens financierde , die binnen de vorige regering nog \" aandacht \" vroeg voor de zaak . Het Leven Vandenbussche zelf besloot dat het hof \" de politieke zeden uit het verleden \" heeft willen veroordelen . \" Het Hof van Cassatie verbrak het arrest zodat het moest worden overgedaan door het hof van beroep van Antwerpen . Dat is het wezen van het arrest dat het Antwerpse hof van beroep woensdag geveld heeft in de zaak van het omstreden tuincentrum Floralux uit Moorslede-Dadizele . Zaakvoerster Christiane Vandenbussche wordt schuldig bevonden aan belangenvermenging ( ' belangenneming ' in juridische taal ) maar krijgt geen straf omdat een straf na al die jaren niet meer zinvol zou zijn . Het is alvast de interpretatie die in de omgeving van Floralux gegeven wordt aan het arrest ( zie pagina 1 ) : dat er een ' nieuwe politieke cultuur ' is aangebroken en de oude politieke zeden weliswaar verwerpelijk waren maar nu niet meer hoeven te worden beboet met , in casu , de sluiting van een zaak . Wat jaren meeging als een omstreden ' CVP-dossier ' krijgt nu door de rechterlijke uitspraak het cachet van een oude koe in de gracht . Om te kunnen voortbestaan wil Floralux dat de landbouwgrond wordt omgezet in kmo-zone . ' Hof veroordeelde politieke zeden van toen ' De advocaat schrijft dat \" ( ... ) het dan duidelijk is dat het ( BPA , sd ) enkel voor private doeleinden , namelijk voor uw firma , werd gerealiseerd en het schepencollege als gangmaker daarvoor fungeert \" . Niettemin hield ze tegenover de VRT-radio vol dat ze \" niets misdaan \" heeft . auteur Verder blijkt dat er overleg over plaatsvond met de liberale partij ( toen nog PVV ) èn met de SP : \" Waltniel ( PVV , sd ) is voor . En Delcroix besloot : \" Zal erdoor komen . publicatiedatum Die context zorgde ervoor dat het dossier steeds gebaad heeft in een sfeer van partijfinanciering in ruil voor regularisering . Algemeen Dossier steeds geassocieerd met partijfinanciering in ruil voor regularisering Floralux verkrijgt een vergunning om een plantenkwekerij op te trekken . \" vrijdag , 2 juni 2000 Het Antwerpse hof weerde het document als bewijsmateriaal maar las het wel voor tijdens de zitting . Wat die politieke zeden waren , valt af te leiden uit de Atoma-schriftjes die Leo Delcroix bijhield toen hij nog CVP-secretaris was . sectie Sybille Decoo Zaakvoerster Floralux houdt vol dat ze niets misdaan heeft Over Floralux stond er : \" Ze willen regularisering . editie "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import Dataset, ClassLabel, Sequence\n",
    "doc = nlp(sample.text)\n",
    "\n",
    "def name_detector_fn(doc):\n",
    "    names = [\"David\", \"Cameron\", \"William\", \"Bos\"]\n",
    "    for token in doc: \n",
    "        if token.text in names:\n",
    "            yield token.i, token.i+1, \"PER\"\n",
    "\n",
    "name_detector = skweak.heuristics.FunctionAnnotator(\"name_detector\", name_detector_fn)\n",
    "\n",
    "geonames = skweak.gazetteers.extract_json_data(\"data/geonames.json\", spacy_model=\"en_core_web_sm\")\n",
    "geonames_annotator = skweak.gazetteers.GazetteerAnnotator(\"locations\", geonames)\n",
    "\n",
    "\n",
    "#hmm = skweak.aggregation.HMM(\"hmm\", [\"PER\", \"ORG\", \"LOC\", \"MISC\"])\n",
    "#hmm.fit_and_aggregate([doc])\n",
    "#spans = doc.spans[\"hmm\"]\n",
    "#doc.ents = spans\n",
    "\n",
    "#for span in doc.ents:\n",
    "#  print(span, span.label, span.start, span.end, span.doc[span.start : span.end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "filled-fifteen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skweak.gazetteers.GazetteerAnnotator"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(geonames_annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "objective-tracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ids', 'ner_tags', 'tokens'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, ClassLabel, Sequence\n",
    "\n",
    "def relabel(ent_label: str) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 label of Spacy labelled entity\n",
    "  \"\"\"\n",
    "  mappings = {\"PERSON\":\"PER\", \"COMPANY\":\"ORG\", \"GPE\":\"LOC\", 'EVENT':\"MISC\", 'FAC':\"MISC\", 'LANGUAGE':\"MISC\", 'LAW':\"MISC\", 'NORP':\"MISC\", 'PRODUCT':\"MISC\",'WORK_OF_ART':\"MISC\", \"MISC\":\"MISC\", \"PER\":\"PER\", \"ORG\":\"ORG\", \"LOC\":\"LOC\"}    \n",
    "  exclude = {\"CARDINAL\", \"ORDINAL\", \"DATE\", \"PERCENT\", \"QUANTITY\", \"TIME\", \"MONEY\"}\n",
    "\n",
    "  return mappings[ent_label] if ent_label != \"\" and ent_label not in exclude else None\n",
    "\n",
    "def convert_ent(token) -> str:\n",
    "  \"\"\"\n",
    "  returns ConLL-2002 IOB style entity label of Spacy token\n",
    "  \"\"\"\n",
    "  return token.ent_iob_ + \"-\" + relabel(token.ent_type_) if relabel(token.ent_type_) else token.ent_iob_\n",
    "\n",
    "def process_spacy(docs: list):\n",
    "  store = []\n",
    "  tokens = []\n",
    "  ids = []\n",
    "\n",
    "  c = 0\n",
    "  classlabels = ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'])\n",
    "  for doc in docs:\n",
    "    ents = [classlabels.str2int(convert_ent(tok)) for tok in doc]\n",
    "    toks = [token.text for token in doc]\n",
    "    store.append(ents)\n",
    "    tokens.append(toks)\n",
    "    ids.append(str(c))\n",
    "    c += 1 \n",
    "    \n",
    "  d = {\"ids\" : ids,\n",
    "       \"ner_tags\" : store,\n",
    "       \"tokens\" : tokens}\n",
    "\n",
    "  class_sequence = Sequence(feature =  classlabels, id = None)\n",
    "  ds = Dataset.from_dict(d)\n",
    "  ds.features[\"ner_tags\"] = class_sequence\n",
    "  return ds\n",
    "\n",
    "ds = process_spacy([doc])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "binding-maker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': '0',\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'tokens': ['De',\n",
       "  'tekst',\n",
       "  'van',\n",
       "  'het',\n",
       "  'arrest',\n",
       "  'is',\n",
       "  'nog',\n",
       "  'niet',\n",
       "  'schriftelijk',\n",
       "  'beschikbaar',\n",
       "  'maar',\n",
       "  'het',\n",
       "  'bericht',\n",
       "  'werd',\n",
       "  'alvast',\n",
       "  'bekendgemaakt',\n",
       "  'door',\n",
       "  'een',\n",
       "  'communicatiebureau',\n",
       "  'dat',\n",
       "  'Floralux',\n",
       "  'inhuurde',\n",
       "  '.',\n",
       "  'In',\n",
       "  \"'\",\n",
       "  '81',\n",
       "  'regulariseert',\n",
       "  'de',\n",
       "  'toenmalige',\n",
       "  'Vlaamse',\n",
       "  'regering',\n",
       "  'de',\n",
       "  'toestand',\n",
       "  'met',\n",
       "  'een',\n",
       "  'BPA',\n",
       "  'dat',\n",
       "  'het',\n",
       "  'bedrijf',\n",
       "  'op',\n",
       "  'eigen',\n",
       "  'kosten',\n",
       "  'heeft',\n",
       "  'laten',\n",
       "  'opstellen',\n",
       "  '.',\n",
       "  'publicatie',\n",
       "  'Vandaag',\n",
       "  'is',\n",
       "  'Floralux',\n",
       "  'dus',\n",
       "  'met',\n",
       "  'alle',\n",
       "  'vergunningen',\n",
       "  'in',\n",
       "  'orde',\n",
       "  ',',\n",
       "  'maar',\n",
       "  'het',\n",
       "  'BPA',\n",
       "  'waarmee',\n",
       "  'die',\n",
       "  'konden',\n",
       "  'verkregen',\n",
       "  'worden',\n",
       "  ',',\n",
       "  'was',\n",
       "  'omstreden',\n",
       "  'omdat',\n",
       "  'zaakvoerster',\n",
       "  'Christiane',\n",
       "  'Vandenbussche',\n",
       "  'haar',\n",
       "  'schepenambt',\n",
       "  'van',\n",
       "  '...',\n",
       "  'In',\n",
       "  'eerste',\n",
       "  'aanleg',\n",
       "  'werd',\n",
       "  'Vandenbussche',\n",
       "  'begin',\n",
       "  'de',\n",
       "  'jaren',\n",
       "  \"'\",\n",
       "  '90',\n",
       "  'veroordeeld',\n",
       "  'wegens',\n",
       "  'belangenvermenging',\n",
       "  'maar',\n",
       "  'later',\n",
       "  'vrijgesproken',\n",
       "  'door',\n",
       "  'het',\n",
       "  'hof',\n",
       "  'van',\n",
       "  'beroep',\n",
       "  'in',\n",
       "  'Gent',\n",
       "  '.',\n",
       "  'SP',\n",
       "  'zit',\n",
       "  'moeilijk',\n",
       "  '--',\n",
       "  '(',\n",
       "  'Eric',\n",
       "  ',',\n",
       "  'sd',\n",
       "  ')',\n",
       "  'Derycke',\n",
       "  'wil',\n",
       "  'BPA',\n",
       "  'goedkeuren',\n",
       "  'op',\n",
       "  'voorwaarde',\n",
       "  'dat',\n",
       "  'BPA',\n",
       "  \"'\",\n",
       "  'De',\n",
       "  'Hoogte',\n",
       "  \"'\",\n",
       "  'beperkt',\n",
       "  'wordt',\n",
       "  'aanvaard',\n",
       "  '(',\n",
       "  'burgemeester',\n",
       "  'van',\n",
       "  'Moorslede',\n",
       "  'Walter',\n",
       "  'Ghekiere',\n",
       "  'zal',\n",
       "  \"'n\",\n",
       "  'voorstel',\n",
       "  'doen',\n",
       "  ')',\n",
       "  '.',\n",
       "  'Onvoldoende',\n",
       "  'om',\n",
       "  'een',\n",
       "  'zware',\n",
       "  'straf',\n",
       "  'uit',\n",
       "  'te',\n",
       "  'spreken',\n",
       "  ',',\n",
       "  'luidt',\n",
       "  'het',\n",
       "  '.',\n",
       "  'Dit',\n",
       "  'hof',\n",
       "  'verbindt',\n",
       "  'nu',\n",
       "  'geen',\n",
       "  'straf',\n",
       "  'aan',\n",
       "  'de',\n",
       "  'schuld',\n",
       "  'die',\n",
       "  'ze',\n",
       "  'vaststelt',\n",
       "  '.',\n",
       "  'Die',\n",
       "  'groeit',\n",
       "  'al',\n",
       "  'snel',\n",
       "  'uit',\n",
       "  'tot',\n",
       "  'een',\n",
       "  'heus',\n",
       "  'tuincentrum',\n",
       "  '.',\n",
       "  'pagina',\n",
       "  'Er',\n",
       "  'worden',\n",
       "  'veel',\n",
       "  'meer',\n",
       "  'zaken',\n",
       "  'verkocht',\n",
       "  'dan',\n",
       "  'je',\n",
       "  'in',\n",
       "  'een',\n",
       "  'plantenkwekerij',\n",
       "  'zou',\n",
       "  'verwachten',\n",
       "  '.',\n",
       "  'Dat',\n",
       "  'blijkt',\n",
       "  'overduidelijk',\n",
       "  'uit',\n",
       "  'een',\n",
       "  'fax',\n",
       "  'die',\n",
       "  'haar',\n",
       "  'advocaat',\n",
       "  'destijds',\n",
       "  'aan',\n",
       "  'haar',\n",
       "  'schreef',\n",
       "  'en',\n",
       "  'die',\n",
       "  'bij',\n",
       "  'een',\n",
       "  'huiszoeking',\n",
       "  'in',\n",
       "  'beslag',\n",
       "  'genomen',\n",
       "  'werd',\n",
       "  '.',\n",
       "  'Het',\n",
       "  'dossier',\n",
       "  'gaat',\n",
       "  'terug',\n",
       "  'tot',\n",
       "  'eind',\n",
       "  'de',\n",
       "  'jaren',\n",
       "  \"'\",\n",
       "  '80',\n",
       "  '.',\n",
       "  'Sybille',\n",
       "  'Decoo',\n",
       "  'Ruimtelijke',\n",
       "  'Ordening',\n",
       "  'zou',\n",
       "  'hebben',\n",
       "  'misbruikt',\n",
       "  'om',\n",
       "  'het',\n",
       "  'BPA',\n",
       "  'te',\n",
       "  'verkrijgen',\n",
       "  '.',\n",
       "  'Het',\n",
       "  'enige',\n",
       "  'bewijs',\n",
       "  'dat',\n",
       "  'overbleef',\n",
       "  'om',\n",
       "  'Vandenbussche',\n",
       "  'te',\n",
       "  'veroordelen',\n",
       "  'voor',\n",
       "  \"'\",\n",
       "  'belangenneming',\n",
       "  \"'\",\n",
       "  'was',\n",
       "  'haar',\n",
       "  'aanwezigheid',\n",
       "  'op',\n",
       "  'het',\n",
       "  'schepencollege',\n",
       "  'toen',\n",
       "  'het',\n",
       "  'dossier-Floralux',\n",
       "  'aan',\n",
       "  'bod',\n",
       "  'kwam',\n",
       "  '.',\n",
       "  'Eind',\n",
       "  \"'\",\n",
       "  '98',\n",
       "  'maakt',\n",
       "  'de',\n",
       "  'Vlaamse',\n",
       "  'regering',\n",
       "  'deze',\n",
       "  'regularisatie',\n",
       "  'definitief',\n",
       "  'met',\n",
       "  'een',\n",
       "  'gewestplanwijziging',\n",
       "  '.',\n",
       "  'Schuld',\n",
       "  'maar',\n",
       "  'geen',\n",
       "  'boete',\n",
       "  '.',\n",
       "  '\"',\n",
       "  'Zelf',\n",
       "  'deed',\n",
       "  'hij',\n",
       "  'dit',\n",
       "  'af',\n",
       "  'als',\n",
       "  '\"',\n",
       "  'larie',\n",
       "  'en',\n",
       "  'apekool',\n",
       "  'van',\n",
       "  'kwatongen',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'De',\n",
       "  'Morgen',\n",
       "  'Vijgen',\n",
       "  'na',\n",
       "  'Pasen',\n",
       "  ',',\n",
       "  'zo',\n",
       "  'lijkt',\n",
       "  'het',\n",
       "  'hof',\n",
       "  'van',\n",
       "  'beroep',\n",
       "  'te',\n",
       "  'hebben',\n",
       "  'gedacht',\n",
       "  '.',\n",
       "  'Antwerpen',\n",
       "  '/',\n",
       "  'Brussel',\n",
       "  'Plaatselijk',\n",
       "  'wordt',\n",
       "  'vermoed',\n",
       "  'dat',\n",
       "  'Floralux',\n",
       "  'de',\n",
       "  'kiescampagnes',\n",
       "  'van',\n",
       "  'Luc',\n",
       "  'Martens',\n",
       "  'financierde',\n",
       "  ',',\n",
       "  'die',\n",
       "  'binnen',\n",
       "  'de',\n",
       "  'vorige',\n",
       "  'regering',\n",
       "  'nog',\n",
       "  '\"',\n",
       "  'aandacht',\n",
       "  '\"',\n",
       "  'vroeg',\n",
       "  'voor',\n",
       "  'de',\n",
       "  'zaak',\n",
       "  '.',\n",
       "  'Het',\n",
       "  'Leven',\n",
       "  'Vandenbussche',\n",
       "  'zelf',\n",
       "  'besloot',\n",
       "  'dat',\n",
       "  'het',\n",
       "  'hof',\n",
       "  '\"',\n",
       "  'de',\n",
       "  'politieke',\n",
       "  'zeden',\n",
       "  'uit',\n",
       "  'het',\n",
       "  'verleden',\n",
       "  '\"',\n",
       "  'heeft',\n",
       "  'willen',\n",
       "  'veroordelen',\n",
       "  '.',\n",
       "  '\"',\n",
       "  'Het',\n",
       "  'Hof',\n",
       "  'van',\n",
       "  'Cassatie',\n",
       "  'verbrak',\n",
       "  'het',\n",
       "  'arrest',\n",
       "  'zodat',\n",
       "  'het',\n",
       "  'moest',\n",
       "  'worden',\n",
       "  'overgedaan',\n",
       "  'door',\n",
       "  'het',\n",
       "  'hof',\n",
       "  'van',\n",
       "  'beroep',\n",
       "  'van',\n",
       "  'Antwerpen',\n",
       "  '.',\n",
       "  'Dat',\n",
       "  'is',\n",
       "  'het',\n",
       "  'wezen',\n",
       "  'van',\n",
       "  'het',\n",
       "  'arrest',\n",
       "  'dat',\n",
       "  'het',\n",
       "  'Antwerpse',\n",
       "  'hof',\n",
       "  'van',\n",
       "  'beroep',\n",
       "  'woensdag',\n",
       "  'geveld',\n",
       "  'heeft',\n",
       "  'in',\n",
       "  'de',\n",
       "  'zaak',\n",
       "  'van',\n",
       "  'het',\n",
       "  'omstreden',\n",
       "  'tuincentrum',\n",
       "  'Floralux',\n",
       "  'uit',\n",
       "  'Moorslede-Dadizele',\n",
       "  '.',\n",
       "  'Zaakvoerster',\n",
       "  'Christiane',\n",
       "  'Vandenbussche',\n",
       "  'wordt',\n",
       "  'schuldig',\n",
       "  'bevonden',\n",
       "  'aan',\n",
       "  'belangenvermenging',\n",
       "  '(',\n",
       "  \"'\",\n",
       "  'belangenneming',\n",
       "  \"'\",\n",
       "  'in',\n",
       "  'juridische',\n",
       "  'taal',\n",
       "  ')',\n",
       "  'maar',\n",
       "  'krijgt',\n",
       "  'geen',\n",
       "  'straf',\n",
       "  'omdat',\n",
       "  'een',\n",
       "  'straf',\n",
       "  'na',\n",
       "  'al',\n",
       "  'die',\n",
       "  'jaren',\n",
       "  'niet',\n",
       "  'meer',\n",
       "  'zinvol',\n",
       "  'zou',\n",
       "  'zijn',\n",
       "  '.',\n",
       "  'Het',\n",
       "  'is',\n",
       "  'alvast',\n",
       "  'de',\n",
       "  'interpretatie',\n",
       "  'die',\n",
       "  'in',\n",
       "  'de',\n",
       "  'omgeving',\n",
       "  'van',\n",
       "  'Floralux',\n",
       "  'gegeven',\n",
       "  'wordt',\n",
       "  'aan',\n",
       "  'het',\n",
       "  'arrest',\n",
       "  '(',\n",
       "  'zie',\n",
       "  'pagina',\n",
       "  '1',\n",
       "  ')',\n",
       "  ':',\n",
       "  'dat',\n",
       "  'er',\n",
       "  'een',\n",
       "  \"'\",\n",
       "  'nieuwe',\n",
       "  'politieke',\n",
       "  'cultuur',\n",
       "  \"'\",\n",
       "  'is',\n",
       "  'aangebroken',\n",
       "  'en',\n",
       "  'de',\n",
       "  'oude',\n",
       "  'politieke',\n",
       "  'zeden',\n",
       "  'weliswaar',\n",
       "  'verwerpelijk',\n",
       "  'waren',\n",
       "  'maar',\n",
       "  'nu',\n",
       "  'niet',\n",
       "  'meer',\n",
       "  'hoeven',\n",
       "  'te',\n",
       "  'worden',\n",
       "  'beboet',\n",
       "  'met',\n",
       "  ',',\n",
       "  'in',\n",
       "  'casu',\n",
       "  ',',\n",
       "  'de',\n",
       "  'sluiting',\n",
       "  'van',\n",
       "  'een',\n",
       "  'zaak',\n",
       "  '.',\n",
       "  'Wat',\n",
       "  'jaren',\n",
       "  'meeging',\n",
       "  'als',\n",
       "  'een',\n",
       "  'omstreden',\n",
       "  \"'\",\n",
       "  'CVP-dossier',\n",
       "  \"'\",\n",
       "  'krijgt',\n",
       "  'nu',\n",
       "  'door',\n",
       "  'de',\n",
       "  'rechterlijke',\n",
       "  'uitspraak',\n",
       "  'het',\n",
       "  'cachet',\n",
       "  'van',\n",
       "  'een',\n",
       "  'oude',\n",
       "  'koe',\n",
       "  'in',\n",
       "  'de',\n",
       "  'gracht',\n",
       "  '.',\n",
       "  'Om',\n",
       "  'te',\n",
       "  'kunnen',\n",
       "  'voortbestaan',\n",
       "  'wil',\n",
       "  'Floralux',\n",
       "  'dat',\n",
       "  'de',\n",
       "  'landbouwgrond',\n",
       "  'wordt',\n",
       "  'omgezet',\n",
       "  'in',\n",
       "  'kmo-zone',\n",
       "  '.',\n",
       "  \"'\",\n",
       "  'Hof',\n",
       "  'veroordeelde',\n",
       "  'politieke',\n",
       "  'zeden',\n",
       "  'van',\n",
       "  'toen',\n",
       "  \"'\",\n",
       "  'De',\n",
       "  'advocaat',\n",
       "  'schrijft',\n",
       "  'dat',\n",
       "  '\"',\n",
       "  '(',\n",
       "  '...',\n",
       "  ')',\n",
       "  'het',\n",
       "  'dan',\n",
       "  'duidelijk',\n",
       "  'is',\n",
       "  'dat',\n",
       "  'het',\n",
       "  '(',\n",
       "  'BPA',\n",
       "  ',',\n",
       "  'sd',\n",
       "  ')',\n",
       "  'enkel',\n",
       "  'voor',\n",
       "  'private',\n",
       "  'doeleinden',\n",
       "  ',',\n",
       "  'namelijk',\n",
       "  'voor',\n",
       "  'uw',\n",
       "  'firma',\n",
       "  ',',\n",
       "  'werd',\n",
       "  'gerealiseerd',\n",
       "  'en',\n",
       "  'het',\n",
       "  'schepencollege',\n",
       "  'als',\n",
       "  'gangmaker',\n",
       "  'daarvoor',\n",
       "  'fungeert',\n",
       "  '\"',\n",
       "  '.',\n",
       "  'Niettemin',\n",
       "  'hield',\n",
       "  'ze',\n",
       "  'tegenover',\n",
       "  'de',\n",
       "  'VRT-radio',\n",
       "  'vol',\n",
       "  'dat',\n",
       "  'ze',\n",
       "  '\"',\n",
       "  'niets',\n",
       "  'misdaan',\n",
       "  '\"',\n",
       "  'heeft',\n",
       "  '.',\n",
       "  'auteur',\n",
       "  'Verder',\n",
       "  'blijkt',\n",
       "  'dat',\n",
       "  'er',\n",
       "  'overleg',\n",
       "  'over',\n",
       "  'plaatsvond',\n",
       "  'met',\n",
       "  'de',\n",
       "  'liberale',\n",
       "  'partij',\n",
       "  '(',\n",
       "  'toen',\n",
       "  'nog',\n",
       "  'PVV',\n",
       "  ')',\n",
       "  'èn',\n",
       "  'met',\n",
       "  'de',\n",
       "  'SP',\n",
       "  ':',\n",
       "  '\"',\n",
       "  'Waltniel',\n",
       "  '(',\n",
       "  'PVV',\n",
       "  ',',\n",
       "  'sd',\n",
       "  ')',\n",
       "  'is',\n",
       "  'voor',\n",
       "  '.',\n",
       "  'En',\n",
       "  'Delcroix',\n",
       "  'besloot',\n",
       "  ':',\n",
       "  '\"',\n",
       "  'Zal',\n",
       "  'erdoor',\n",
       "  'komen',\n",
       "  '.',\n",
       "  'publicatiedatum',\n",
       "  'Die',\n",
       "  'context',\n",
       "  'zorgde',\n",
       "  'ervoor',\n",
       "  'dat',\n",
       "  'het',\n",
       "  'dossier',\n",
       "  'steeds',\n",
       "  'gebaad',\n",
       "  'heeft',\n",
       "  'in',\n",
       "  'een',\n",
       "  'sfeer',\n",
       "  'van',\n",
       "  'partijfinanciering',\n",
       "  'in',\n",
       "  'ruil',\n",
       "  'voor',\n",
       "  'regularisering',\n",
       "  '.',\n",
       "  'Algemeen',\n",
       "  'Dossier',\n",
       "  'steeds',\n",
       "  'geassocieerd',\n",
       "  'met',\n",
       "  'partijfinanciering',\n",
       "  'in',\n",
       "  'ruil',\n",
       "  'voor',\n",
       "  'regularisering',\n",
       "  'Floralux',\n",
       "  'verkrijgt',\n",
       "  'een',\n",
       "  'vergunning',\n",
       "  'om',\n",
       "  'een',\n",
       "  'plantenkwekerij',\n",
       "  'op',\n",
       "  'te',\n",
       "  'trekken',\n",
       "  '.',\n",
       "  '\"',\n",
       "  'vrijdag',\n",
       "  ',',\n",
       "  '2',\n",
       "  'juni',\n",
       "  '2000',\n",
       "  'Het',\n",
       "  'Antwerpse',\n",
       "  'hof',\n",
       "  'weerde',\n",
       "  'het',\n",
       "  'document',\n",
       "  'als',\n",
       "  'bewijsmateriaal',\n",
       "  'maar',\n",
       "  'las',\n",
       "  'het',\n",
       "  'wel',\n",
       "  'voor',\n",
       "  'tijdens',\n",
       "  'de',\n",
       "  'zitting',\n",
       "  '.',\n",
       "  'Wat',\n",
       "  'die',\n",
       "  'politieke',\n",
       "  'zeden',\n",
       "  'waren',\n",
       "  ',',\n",
       "  'valt',\n",
       "  'af',\n",
       "  'te',\n",
       "  'leiden',\n",
       "  'uit',\n",
       "  'de',\n",
       "  'Atoma-schriftjes',\n",
       "  'die',\n",
       "  'Leo',\n",
       "  'Delcroix',\n",
       "  'bijhield',\n",
       "  'toen',\n",
       "  'hij',\n",
       "  'nog',\n",
       "  'CVP-secretaris',\n",
       "  'was',\n",
       "  '.',\n",
       "  'sectie',\n",
       "  'Sybille',\n",
       "  'Decoo',\n",
       "  'Zaakvoerster',\n",
       "  'Floralux',\n",
       "  'houdt',\n",
       "  'vol',\n",
       "  'dat',\n",
       "  'ze',\n",
       "  'niets',\n",
       "  'misdaan',\n",
       "  'heeft',\n",
       "  'Over',\n",
       "  'Floralux',\n",
       "  'stond',\n",
       "  'er',\n",
       "  ':',\n",
       "  '\"',\n",
       "  'Ze',\n",
       "  'willen',\n",
       "  'regularisering',\n",
       "  '.',\n",
       "  'editie']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b361823-c157-4bf7-ae16-4d3105914fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# We retrieve the texts from Wablieft tarfile\n",
    "texts = [] \n",
    "archive_file = tarfile.open(\"data/plain.tar.xz\")\n",
    "for archive_member in archive_file.getnames():\n",
    "    if archive_member.endswith(\".txt\"):\n",
    "        #weird encoding because of Turkish letters\n",
    "        text = archive_file.extractfile(archive_member).read().decode(\"cp850\")\n",
    "        texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc8d9ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/geonames.json\n",
      "Populating trie for class GPE (number: 15205)\n"
     ]
    }
   ],
   "source": [
    "def name_detector_fn(doc):\n",
    "    names = [\"David\", \"Cameron\", \"William\", \"Bos\"]\n",
    "    for token in doc: \n",
    "        if token.text in names:\n",
    "            yield token.i, token.i+1, \"PER\"\n",
    "\n",
    "name_detector = skweak.heuristics.FunctionAnnotator(\"name_detector\", name_detector_fn)\n",
    "\n",
    "def address_detector_fn(doc):\n",
    "    for token in doc: \n",
    "        if token.text[0].isupper() and doc[token.i+1].is_digit:\n",
    "            yield token.i, token.i+2, \"LOCATION\"\n",
    "\n",
    "address_detector = skweak.heuristics.FunctionAnnotator(\"address_detector\", address_detector_fn)\n",
    "\n",
    "def company_detector_fn(doc):\n",
    "    companies = [\"Microsoft\", \"Apple\", \"Gemeente Amsterdam\", \"Universiteit van Amsterdam\", \"UvA\"]\n",
    "    for token in doc:\n",
    "        if token.text in companies:\n",
    "            yield token.i, token.i+1, \"ORG\"\n",
    "\n",
    "company_detector = skweak.heuristics.FunctionAnnotator(\"company_detector\", company_detector_fn)\n",
    "\n",
    "names = skweak.gazetteers.extract_json_data(\"data/geonames.json\", spacy_model=\"en_core_web_sm\")\n",
    "name_annotator = skweak.gazetteers.GazetteerAnnotator(\"locations\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "naval-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(address_detector) == skweak.heuristics.FunctionAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = list(name_detector.pipe(name_annotator.pipe(docs[:25])))\n",
    "hmm = skweak.aggregation.HMM(\"hmm\", [\"PERSON\", \"LOCATION\", \"ORG\", \"GPE\"])\n",
    "\n",
    "#skweak.utils.display_entities(processed[7], \"locations\")\n",
    "hmm.fit_and_aggregate(processed)\n",
    "skweak.utils.display_entities(processed[7], \"hmm\", add_tooltip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb882579-f2b5-42b7-949b-acd14f6bc932",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in processed:\n",
    "    if \"hmm\" in doc.spans.keys():\n",
    "        doc.ents = doc.spans[\"hmm\"]\n",
    "    else: \n",
    "        doc.ents = ()\n",
    "\n",
    "docs = docs[:100]\n",
    "skweak.utils.docbin_writer(docs, \"data/wablieft.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e7409-4b44-4ca5-98c6-c1acc49e1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b9864-0173-49fa-b912-601574d3e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spacy train config.cfg --paths.train data/wablieft.spacy --paths.dev data/wablieft.spacy --initialize.vectors nl_core_news_md --output output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d60f36-2ee5-4ace-8004-cef7f35e33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load(\"output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4866f2-3b2b-4bea-914d-39e74acb1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = processed[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe064c-6b76-40e6-b25e-3b177976a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "skweak.utils.display_entities(model(\"Hoi ik ben William en ik woon in New York. Ik kom uit Flanders en ben geboren in Antwerpen.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990abbc5-e408-4044-8038-bacb93a0f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lison2020 import annotations\n",
    "from spacy.tokens import DocBin\n",
    "test = DocBin().from_disk(\"data/ned_testb.spacy\")\n",
    "sys.path.insert(0, './lison2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457143e0-71a1-42ea-9665-1b692bc93f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = annotations.FullAnnotator().add_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066d396-ada1-421c-9fef-35eb03d8e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598172f-9de8-483f-9a07-b35e0f721559",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.spacy_benchmark(\"data/ned_testb.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7940a0-c9f8-4149-a8df-f217bd693862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
